{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup in GCP\n",
    "- apt-get --purge remove \"cublas\" \"cuda*\"\n",
    "- reboot\n",
    "- sudo curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
    "- sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
    "- sudo apt-get install cuda-10-1\n",
    "- pip3 install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "- pip3 install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "- pip3 install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "- pip3 install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "- pip3 install torch-geometric\n",
    "\n",
    "** Both the PyTorch and torch_sparse CUDA version must matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import torch; print(torch.version.cuda)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.243\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all required arguments\n",
    "class gnn_args:\n",
    "    def __init__(self):\n",
    "        self.datadir = \"data\"        # Directory where benchmark is stored (io_parser)\n",
    "        self.logdir = \"log\"          # Tensorboard log directory\n",
    "        self.ckptdir = \"ckpt\"        # Model checkpoint directory\n",
    "        self.dataset = \"BAGraph\"     # Synthetic dataset, syn1\n",
    "        self.opt = \"adam\"            # opt_parser\n",
    "        self.opt_scheduler = \"none\"  # Optimizer scheduler\n",
    "        self.max_nodes = 100         # Maximum number of nodes\n",
    "                                     # (ignore graphs with nodes exceeding the number)\n",
    "        self.cuda = \"0\"              # CUDA value\n",
    "        self.feature_type = \"default\"# Feature used for encoder with possible values : id, deg\n",
    "        self.lr = 0.001              # Learning rate\n",
    "        self.clip = 2.0\n",
    "        \n",
    "        self.batch_size = 20         # Batch size\n",
    "        self.num_epochs = 1000       # Number of epochs to train data\n",
    "        self.train_ratio = 0.8       # Ratio of number of training set to all graphs\n",
    "        self.test_ratio = 0.1\n",
    "        self.num_workers = 1         # Number of workers to load data\n",
    "        self.input_dim = 10          # Input feature dimension\n",
    "        self.hidden_dim = 20         # Hidden layer dimension\n",
    "        self.output_dim = 20         # Output layer dimension\n",
    "        self.num_classes = 2         # Number of label classes\n",
    "        self.num_gc_layers = 3       # Number of graph convolution layers before each pooling\n",
    "        \n",
    "        self.dropout = 0.0           # Dropout rate\n",
    "        self.weight_decay = 0.005    # Weight decay regularization constant\n",
    "        self.method = \"base\"         # Method used with possible values : base\n",
    "        self.name_suffix = \"\"        # Suffix added to the output filename\n",
    "        self.assign_ratio = 0.1      # Ratio of number of nodes in consecutive layers\n",
    "        \n",
    "        self.bias = True             # \"Whether to add bias\n",
    "        \n",
    "        self.gpu = False             # Whether to use GPU\n",
    "        self.linkpred = False        # Whether link prediction side objective is used\n",
    "        self.bn = False              # Whether batch normalization is used\n",
    "        self.bmname = None           # Name of the benchmark datase\n",
    "\n",
    "prog_args = gnn_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start with these parsed program arguments :\n",
      " <__main__.gnn_args object at 0x7fa2b8f32630>\n",
      "Values in Constant Feature Generator :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Constant feature generator :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "------ Building the Synthetic BA graph with 'House' motifs ------\n",
      "Role Id of the BA graph :\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Indicator of the id of the next node : 20\n",
      "Number of nodes in the BA graph :  20\n",
      "Number of motifs :  3\n",
      "List of shapes : [['house'], ['house'], ['house']]\n",
      "No. of shapes : 3\n",
      "Spacing :  6\n",
      "Plugins :  [0, 6, 12]\n",
      "seen_shapes :  {'basis': [0, 20]}\n",
      "\n",
      "-----------------------------------------\n",
      "Shape_ID : 0 with shape type : house\n",
      "1 shapes with list of Shape : ['house']\n",
      "The shape starts from index 1 :  []\n",
      "\n",
      "The list of arguments : [20, 0]\n",
      "The first item in list of arguments : 20\n",
      "The second item in list of arguments : 0\n",
      "Column start : 1\n",
      "Observe seen_shapes :  {'basis': [0, 20], 'house': [1, 5]}\n",
      "Labels of BA graph with attached motifs :\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 3]\n",
      "No. of nodes in attached graph :  25\n",
      "With attached motif nodes, index starts from :  25\n",
      "\n",
      "-----------------------------------------\n",
      "Shape_ID : 1 with shape type : house\n",
      "1 shapes with list of Shape : ['house']\n",
      "The shape starts from index 1 :  []\n",
      "\n",
      "The list of arguments : [25, 0]\n",
      "The first item in list of arguments : 25\n",
      "The second item in list of arguments : 0\n",
      "Column start : 1\n",
      "Observe seen_shapes :  {'basis': [0, 20], 'house': [1, 5]}\n",
      "Labels of BA graph with attached motifs :\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3]\n",
      "No. of nodes in attached graph :  30\n",
      "With attached motif nodes, index starts from :  30\n",
      "\n",
      "-----------------------------------------\n",
      "Shape_ID : 2 with shape type : house\n",
      "1 shapes with list of Shape : ['house']\n",
      "The shape starts from index 1 :  []\n",
      "\n",
      "The list of arguments : [30, 0]\n",
      "The first item in list of arguments : 30\n",
      "The second item in list of arguments : 0\n",
      "Column start : 1\n",
      "Observe seen_shapes :  {'basis': [0, 20], 'house': [1, 5]}\n",
      "Labels of BA graph with attached motifs :\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3]\n",
      "No. of nodes in attached graph :  35\n",
      "With attached motif nodes, index starts from :  35\n",
      "\n",
      "Information of the motif graph :\n",
      " Name: \n",
      "Type: Graph\n",
      "Number of nodes: 5\n",
      "Number of edges: 6\n",
      "Average degree:   2.4000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate node features for 35 nodes.\n",
      "Values of feat_dict[0][\"feat\"]: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Node attributes of node '0', G.nodes[0][\"feat\"]: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "------ Generated the Synthetic BA graph with 'House' motifs ------\n",
      "Name of generated graph : ba_20_3\n",
      "------------ GCNEncoderNode Model ------------\n",
      "Input dimensions : 10\n",
      "Hidden dimensions : 20\n",
      "Output dimensions : 20\n",
      "Number of classes in args : 4\n",
      "Number of GCN layers : 3\n",
      "Method :  base\n",
      "*** Check received batch_size argument : 20\n",
      "*** Batch normalization from caller (default : False) : False\n",
      "GcnEncoderNode model :\n",
      " GcnEncoderNode(\n",
      "  (conv_first): GraphConv()\n",
      "  (conv_block): ModuleList(\n",
      "    (0): GraphConv()\n",
      "  )\n",
      "  (conv_last): GraphConv()\n",
      "  (act): ReLU()\n",
      "  (pred_model): Linear(in_features=60, out_features=4, bias=True)\n",
      "  (celoss): CrossEntropyLoss()\n",
      ")\n",
      "------ Preprocess Input graph ------\n",
      "The shape of the adjacency matrix ('dxd') of input graph : (35, 35)\n",
      "Feature dimensions of the last node '34' : 10\n",
      "The shape of the adjacency matrix after expansion : (1, 35, 35)\n",
      "The shape of the features matrix after expansion : (1, 35, 10)\n",
      "The shape of the labels matrix after expansion : (1, 35)\n",
      "epoch:  0 ; loss:  1.4039500951766968 ; train_acc:  0.14285714285714285 ; test_acc:  0.2857142857142857 ; train_prec:  0.03571428571428571 ; test_prec:  0.09523809523809523 ; epoch time:  0.02\n",
      "epoch:  10 ; loss:  1.3382434844970703 ; train_acc:  0.14285714285714285 ; test_acc:  0.2857142857142857 ; train_prec:  0.03571428571428571 ; test_prec:  0.09523809523809523 ; epoch time:  0.00\n",
      "epoch:  20 ; loss:  1.2804023027420044 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samDev/ml/VirtualEnv/env/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  30 ; loss:  1.231231689453125 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  40 ; loss:  1.1893236637115479 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  50 ; loss:  1.1540679931640625 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  60 ; loss:  1.1255548000335693 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  70 ; loss:  1.1028568744659424 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  80 ; loss:  1.0833762884140015 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  90 ; loss:  1.063063621520996 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  100 ; loss:  1.0364583730697632 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  110 ; loss:  0.9949931502342224 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  120 ; loss:  0.9282625317573547 ; train_acc:  0.6071428571428571 ; test_acc:  0.42857142857142855 ; train_prec:  0.15178571428571427 ; test_prec:  0.14285714285714285 ; epoch time:  0.00\n",
      "epoch:  130 ; loss:  0.8389403223991394 ; train_acc:  0.6785714285714286 ; test_acc:  0.5714285714285714 ; train_prec:  0.41346153846153844 ; test_prec:  0.5 ; epoch time:  0.00\n",
      "epoch:  140 ; loss:  0.7617984414100647 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.33479532163742687 ; test_prec:  0.47222222222222215 ; epoch time:  0.00\n",
      "epoch:  150 ; loss:  0.703610897064209 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.33479532163742687 ; test_prec:  0.47222222222222215 ; epoch time:  0.00\n",
      "epoch:  160 ; loss:  0.6570630073547363 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.33479532163742687 ; test_prec:  0.47222222222222215 ; epoch time:  0.00\n",
      "epoch:  170 ; loss:  0.6222957372665405 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.34090909090909094 ; test_prec:  0.5 ; epoch time:  0.00\n",
      "epoch:  180 ; loss:  0.593110978603363 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.34090909090909094 ; test_prec:  0.5 ; epoch time:  0.00\n",
      "epoch:  190 ; loss:  0.5662243962287903 ; train_acc:  0.75 ; test_acc:  0.7142857142857143 ; train_prec:  0.34090909090909094 ; test_prec:  0.5 ; epoch time:  0.00\n",
      "epoch:  200 ; loss:  0.5421268343925476 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  210 ; loss:  0.5218796133995056 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  220 ; loss:  0.5047425031661987 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  230 ; loss:  0.48996132612228394 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  240 ; loss:  0.4770885109901428 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  250 ; loss:  0.46495962142944336 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  260 ; loss:  0.45267727971076965 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  270 ; loss:  0.43962860107421875 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  280 ; loss:  0.4235496520996094 ; train_acc:  0.8214285714285714 ; test_acc:  0.8571428571428571 ; train_prec:  0.6111111111111112 ; test_prec:  0.8888888888888888 ; epoch time:  0.00\n",
      "epoch:  290 ; loss:  0.40160492062568665 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  300 ; loss:  0.37575381994247437 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  310 ; loss:  0.35425513982772827 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  320 ; loss:  0.3355136215686798 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  330 ; loss:  0.31867191195487976 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  340 ; loss:  0.3063998520374298 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  350 ; loss:  0.29041004180908203 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  360 ; loss:  0.27818813920021057 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  370 ; loss:  0.26611682772636414 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  380 ; loss:  0.25550028681755066 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  390 ; loss:  0.24483029544353485 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  400 ; loss:  0.23568664491176605 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  410 ; loss:  0.2262953668832779 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  420 ; loss:  0.21737678349018097 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  430 ; loss:  0.20880822837352753 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  440 ; loss:  0.2007431983947754 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  450 ; loss:  0.19300933182239532 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  460 ; loss:  0.1853548288345337 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  470 ; loss:  0.1780829280614853 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  480 ; loss:  0.17098890244960785 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  490 ; loss:  0.16418048739433289 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  500 ; loss:  0.15731409192085266 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  510 ; loss:  0.15074516832828522 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  520 ; loss:  0.14443589746952057 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  530 ; loss:  0.1382312774658203 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  540 ; loss:  0.13225987553596497 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  550 ; loss:  0.12647788226604462 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  560 ; loss:  0.12107431888580322 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  570 ; loss:  0.1161077544093132 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  580 ; loss:  0.11151180416345596 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  590 ; loss:  0.10719947516918182 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  600 ; loss:  0.10314268618822098 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  610 ; loss:  0.0993209257721901 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  620 ; loss:  0.09568659216165543 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  630 ; loss:  0.0922352597117424 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  640 ; loss:  0.08891289681196213 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  650 ; loss:  0.08575475215911865 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  660 ; loss:  0.08273829519748688 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  670 ; loss:  0.07989990711212158 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  680 ; loss:  0.07712682336568832 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  690 ; loss:  0.07452423870563507 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  700 ; loss:  0.07207953184843063 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  710 ; loss:  0.06962836533784866 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  720 ; loss:  0.06778484582901001 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  730 ; loss:  0.06539591401815414 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  740 ; loss:  0.06325068324804306 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  750 ; loss:  0.0610717348754406 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  760 ; loss:  0.05927104875445366 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  770 ; loss:  0.057411789894104004 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  780 ; loss:  0.05572286248207092 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  790 ; loss:  0.05414837971329689 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  800 ; loss:  0.05268542841076851 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  810 ; loss:  0.05122727155685425 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  820 ; loss:  0.04943281412124634 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  830 ; loss:  0.04801901429891586 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  840 ; loss:  0.04679092392325401 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  850 ; loss:  0.04535924270749092 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  860 ; loss:  0.04433252289891243 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  870 ; loss:  0.04292035847902298 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  880 ; loss:  0.04194578528404236 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  890 ; loss:  0.040674176067113876 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  900 ; loss:  0.03958090767264366 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  910 ; loss:  0.03858401998877525 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  920 ; loss:  0.0377601757645607 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  930 ; loss:  0.0365716852247715 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  940 ; loss:  0.03592870384454727 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  950 ; loss:  0.034952931106090546 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  960 ; loss:  0.033891528844833374 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  970 ; loss:  0.03311775252223015 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  980 ; loss:  0.032242026180028915 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "epoch:  990 ; loss:  0.031441789120435715 ; train_acc:  1.0 ; test_acc:  1.0 ; train_prec:  1.0 ; test_prec:  1.0 ; epoch time:  0.00\n",
      "Confusion Matrix of train result :\n",
      " [[17  0  0  0]\n",
      " [ 0  4  0  0]\n",
      " [ 0  0  4  0]\n",
      " [ 0  0  0  3]]\n",
      "Confusion Matrix of test result :\n",
      " [[3 0 0]\n",
      " [0 2 0]\n",
      " [0 0 2]]\n",
      "Labels of the Computational graph :\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3 1 1 2 2 3 1 1 2 2 3]]\n",
      "Prediction result of the Computational graph :\n",
      " [[[ 3.332701   -1.7189426  -5.147857   -2.3727663 ]\n",
      "  [ 3.4665647  -1.9025738  -4.8109746  -2.5232244 ]\n",
      "  [ 3.3718405  -2.1947322  -3.4699092  -2.49906   ]\n",
      "  [ 3.4675326  -2.2590148  -3.7450867  -2.5032115 ]\n",
      "  [ 3.520472   -2.097652   -4.4505296  -2.5113714 ]\n",
      "  [ 3.3916693  -1.7525692  -4.9966416  -2.5122395 ]\n",
      "  [ 3.3266418  -1.6766102  -5.1448884  -2.4393167 ]\n",
      "  [ 3.4806757  -1.9337585  -4.7741647  -2.5245388 ]\n",
      "  [ 3.5053568  -2.1173592  -4.2590604  -2.5307138 ]\n",
      "  [ 3.5013475  -2.1105323  -4.2613835  -2.5282187 ]\n",
      "  [ 3.4786353  -2.064704   -4.1821737  -2.577097  ]\n",
      "  [ 3.5025983  -2.051344   -4.441217   -2.534398  ]\n",
      "  [ 3.3893085  -1.8480105  -5.097487   -2.331954  ]\n",
      "  [ 3.4979486  -2.1377597  -4.128482   -2.5455143 ]\n",
      "  [ 3.4635024  -2.1674652  -3.9097917  -2.527371  ]\n",
      "  [ 3.4806457  -2.188782   -3.9807909  -2.5047753 ]\n",
      "  [ 3.4684992  -2.2157393  -3.8210952  -2.5220482 ]\n",
      "  [ 3.3085637  -2.0380757  -3.477565   -2.5490208 ]\n",
      "  [ 3.3251402  -2.1780472  -3.2696333  -2.5232685 ]\n",
      "  [ 3.3448029  -2.318225   -3.1513307  -2.462022  ]\n",
      "  [-0.30966106  3.0959668  -1.5355648  -2.6633387 ]\n",
      "  [-0.72920525  2.9009938  -0.7394014  -1.8862379 ]\n",
      "  [-1.5390601   0.5075873   3.8862936   0.35707685]\n",
      "  [-1.2291467   0.23889056  3.8675034   0.22925788]\n",
      "  [-0.40150756 -1.5325377  -0.71396714  2.6570752 ]\n",
      "  [-0.2676067   3.0352397  -1.4157432  -2.768515  ]\n",
      "  [-0.7292426   2.9215376  -0.6309261  -1.9541469 ]\n",
      "  [-1.5390601   0.5075873   3.8862936   0.35707685]\n",
      "  [-1.12333     0.14978054  3.8166432   0.18097752]\n",
      "  [-0.35971165 -1.6755261  -0.5836784   2.701463  ]\n",
      "  [-0.33554277  3.1289377  -1.4647367  -2.6717844 ]\n",
      "  [-0.73901427  2.8977149  -0.7483524  -1.8619719 ]\n",
      "  [-1.5390601   0.5075873   3.8862936   0.35707685]\n",
      "  [-1.2671655   0.26073113  3.8779547   0.26283637]\n",
      "  [-0.4320802  -1.4901909  -0.7000761   2.6542232 ]]]\n",
      "Train index of the Computational graph data :\n",
      " [14, 30, 4, 22, 12, 7, 15, 31, 25, 10, 2, 0, 34, 9, 8, 32, 21, 6, 28, 1, 18, 13, 29, 24, 16, 3, 23, 17]\n",
      "Created filename with path :  ./ckpt/BAGraph_base_hdim20_odim20/BA_graph_model_dict.pth\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "\n",
    "# Running from Jupyter notebook will bypass main() in train.py\n",
    "model = train.syn_task1(prog_args, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv_first.weight \t torch.Size([10, 20])\n",
      "conv_first.bias \t torch.Size([20])\n",
      "conv_block.0.weight \t torch.Size([20, 20])\n",
      "conv_block.0.bias \t torch.Size([20])\n",
      "conv_last.weight \t torch.Size([20, 20])\n",
      "conv_last.bias \t torch.Size([20])\n",
      "pred_model.weight \t torch.Size([4, 60])\n",
      "pred_model.bias \t torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "# Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/io_utils.py\n",
    "import torch\n",
    "\n",
    "'''\n",
    "Load a pre-trained pytorch model from checkpoint.\n",
    "'''\n",
    "def load_ckpt(args, isbest=False):\n",
    "\n",
    "    print(\"Attempt to load model...\")\n",
    "    filename = create_filename(args.ckptdir, args, isbest)\n",
    "    print(\"Loading file : \", filename)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        ckpt = torch.load(filename)\n",
    "    else:\n",
    "        print(\"Checkpoint does not exist!\")\n",
    "        print(\"Check correct path for : {}\".format(filename))\n",
    "        print(\"Make sure you have provided the correct path!\")\n",
    "        print(\"Or you may have forgotten to train a model for this dataset.\")\n",
    "        print()\n",
    "        print(\"To train one of the models, run the following\")\n",
    "        print(\">> python train.py --dataset=DATASET_NAME\")\n",
    "        print()\n",
    "        raise Exception(\"File is not found.\")\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn-model-explainer/utils/io_utils.py\n",
    "import os\n",
    "\n",
    "'''\n",
    "Generate label prefix for a graph model.\n",
    "'''\n",
    "def gen_prefix(args):\n",
    "    if args.bmname is not None:\n",
    "        name = args.bmname\n",
    "    else:\n",
    "        name = args.dataset\n",
    "    name += \"_\" + args.method\n",
    "\n",
    "    name += \"_hdim\" + str(args.hidden_dim) + \"_odim\" + str(args.output_dim)\n",
    "    if not args.bias:\n",
    "        name += \"_nobias\"\n",
    "    if len(args.name_suffix) > 0:\n",
    "        name += \"_\" + args.name_suffix\n",
    "    return name\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create filename for saving.\n",
    "\n",
    "Args:\n",
    "    args        :  the arguments parsed in the parser\n",
    "    isbest      :  whether the saved model is the best-performing one\n",
    "    num_epochs  :  epoch number of the model (when isbest=False)\n",
    "\"\"\"\n",
    "def create_filename(save_dir, args, isbest=False, num_epochs=-1):\n",
    "    filename = os.path.join(\"./\", save_dir, gen_prefix(args))\n",
    "    os.makedirs(filename, exist_ok=True)\n",
    "\n",
    "    if isbest:\n",
    "        filename = os.path.join(filename, \"best\")\n",
    "    elif num_epochs > 0:\n",
    "        filename = os.path.join(filename, str(num_epochs))\n",
    "    else:\n",
    "        filename = os.path.join(filename, \"BA_graph\")\n",
    "\n",
    "    path_filename = filename + \"_model_dict.pth\" # \".pth.tar\"\n",
    "    print(\"Created filename with path : \", path_filename)\n",
    "    return path_filename\n",
    "\n",
    "\"\"\"\n",
    "Save pytorch model checkpoint.\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "model         : The PyTorch model to save.\n",
    "optimizer     : The optimizer used to train the model.\n",
    "args          : A dict of meta-data about the model.\n",
    "num_epochs    : Number of training epochs.\n",
    "isbest        : True if the model has the highest accuracy so far.\n",
    "cg_dict       : A dictionary of the sampled computation graphs.\n",
    "\n",
    "Output :\n",
    "----------------------------------------------------------------------------------------\n",
    "filename      : File saved in \"ckpt\" subdirectory\n",
    "\"\"\"\n",
    "def save_checkpoint(model, optimizer, args, num_epochs=-1, isbest=False, cg_dict=None):\n",
    "    filename = create_filename(args.ckptdir, args, isbest, num_epochs=num_epochs)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": num_epochs,\n",
    "            \"model_type\": args.method,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"cg\": cg_dict,\n",
    "        },\n",
    "        filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to load model...\n",
      "Created filename with path :  ./ckpt/BAGraph_base_hdim20_odim20/BA_graph_model_dict.pth\n",
      "Loading file :  ./ckpt/BAGraph_base_hdim20_odim20/BA_graph_model_dict.pth\n",
      "=> loading checkpoint './ckpt/BAGraph_base_hdim20_odim20/BA_graph_model_dict.pth'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.001,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0.0,\n",
       "  'amsgrad': False,\n",
       "  'params': [140334513508568,\n",
       "   140334513508784,\n",
       "   140334557318168,\n",
       "   140334557315648,\n",
       "   140334557318456,\n",
       "   140334557318744,\n",
       "   140334522769128,\n",
       "   140334513595808]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnn-model-explainer/explainer_main.py \n",
    "model_dict = load_ckpt(prog_args)\n",
    "model_optimizer = model_dict['optimizer']\n",
    "model_optimizer.state_dict()['param_groups']\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in loaded model dictionary : ['epoch', 'model_type', 'optimizer', 'model_state', 'optimizer_state', 'cg']\n",
      "Keys in loaded model optimizer dictionary: ['state', 'param_groups']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Keys in loaded model dictionary :\",list(model_dict))\n",
    "print(\"Keys in loaded model optimizer dictionary:\",list(model_optimizer.state_dict()))\n",
    "\n",
    "model_dict['cg']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from subdirectory \"ckpt\" ...\n",
      "input dim:  10 ; num classes:  4\n"
     ]
    }
   ],
   "source": [
    "cg_dict = model_dict['cg']\n",
    "input_dim = cg_dict['feat'].shape[2] \n",
    "num_classes = cg_dict['pred'].shape[2]\n",
    "print(\"Loaded model from subdirectory \\\"{}\\\" ...\".format(prog_args.ckptdir))\n",
    "print(\"input dim: \", input_dim, \"; num classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default graph mode : False\n",
      "Multigraph class : -1\n",
      "Graph Index : -1\n",
      "Explainer graph mode : False\n",
      "Input dimension : 10\n",
      "Hidden dimension : 20\n",
      "Output dimension : 20\n",
      "Number of classes : 4\n",
      "Number of GCN layers : 3\n",
      "Batch Normalization : False\n",
      "*** Check received batch_size argument : 20\n",
      "*** Batch normalization from caller (default : False) : False\n",
      "\n",
      "GcnEncoderNode model :\n",
      " GcnEncoderNode(\n",
      "  (conv_first): GraphConv()\n",
      "  (conv_block): ModuleList(\n",
      "    (0): GraphConv()\n",
      "  )\n",
      "  (conv_last): GraphConv()\n",
      "  (act): ReLU()\n",
      "  (pred_model): Linear(in_features=60, out_features=4, bias=True)\n",
      "  (celoss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "\n",
    "prog_args.graph_mode = False\n",
    "prog_args.multigraph_class=-1\n",
    "prog_args.graph_idx=-1\n",
    "prog_args.explain_node = None\n",
    "\n",
    "print(\"Default graph mode :\", prog_args.graph_mode)\n",
    "\n",
    "# Determine explainer mode\n",
    "graph_mode = (\n",
    "    prog_args.graph_mode\n",
    "    or prog_args.multigraph_class >= 0\n",
    "    or prog_args.graph_idx >= 0\n",
    ")\n",
    "\n",
    "print(\"Multigraph class :\", prog_args.multigraph_class)\n",
    "print(\"Graph Index :\", prog_args.graph_idx)\n",
    "print(\"Explainer graph mode :\", graph_mode)\n",
    "print(\"Input dimension :\", input_dim)\n",
    "print(\"Hidden dimension :\", prog_args.hidden_dim)\n",
    "print(\"Output dimension :\", prog_args.output_dim)\n",
    "print(\"Number of classes :\", num_classes)\n",
    "print(\"Number of GCN layers :\", prog_args.num_gc_layers)\n",
    "print(\"Batch Normalization :\", prog_args.bn)\n",
    "\n",
    "model = models.GcnEncoderNode(input_dim=input_dim,\n",
    "                              hidden_dim=prog_args.hidden_dim,\n",
    "                              embedding_dim=prog_args.output_dim,\n",
    "                              label_dim=num_classes,\n",
    "                              num_layers=prog_args.num_gc_layers,\n",
    "                              bn=prog_args.bn,\n",
    "                              args=prog_args)\n",
    "\n",
    "print(\"\\nGcnEncoderNode model :\\n\", model)\n",
    "#GcnEncoderNode(\n",
    "#  (conv_first): GraphConv()\n",
    "#  (conv_block): ModuleList(\n",
    "#    (0): GraphConv()\n",
    "#  )\n",
    "#  (conv_last): GraphConv()\n",
    "#  (act): ReLU()\n",
    "#  (pred_model): Linear(in_features=60, out_features=4, bias=True)\n",
    "#  (celoss): CrossEntropyLoss()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load state_dict (obtained by model.state_dict() when saving checkpoint)\n",
    "# Loading Model for Inference\n",
    "model.load_state_dict(model_dict['model_state']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data stored in computational graph dictionary\n",
    "data = cg_dict\n",
    "data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn-model-explainer/explainer/explain.py\n",
    "\"\"\" explain.py\n",
    "    Implementation of the explainer. \n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorboardX.utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import pdb\n",
    "\n",
    "import utils.io_utils as io_utils\n",
    "#import utils.train_utils as train_utils\n",
    "#import utils.graph_utils as graph_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class Explainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        adj,\n",
    "        feat,\n",
    "        label,\n",
    "        pred,\n",
    "        train_idx,\n",
    "        args,\n",
    "        writer=None,\n",
    "        print_training=True,\n",
    "        graph_mode=False,\n",
    "        graph_idx=False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.adj = adj\n",
    "        self.feat = feat\n",
    "        self.label = label\n",
    "        self.pred = pred\n",
    "        self.train_idx = train_idx\n",
    "        self.n_hops = args.num_gc_layers\n",
    "        self.graph_mode = graph_mode\n",
    "        self.graph_idx = graph_idx\n",
    "        # graph_utils.neighborhoods\n",
    "        self.neighborhoods = None if self.graph_mode else get_neighborhoods(adj=self.adj, n_hops=self.n_hops, use_cuda=use_cuda)\n",
    "        self.args = args\n",
    "        self.writer = writer\n",
    "        self.print_training = print_training\n",
    "\n",
    "    \n",
    "    # Main method\n",
    "    def explain(\n",
    "        self, node_idx, graph_idx=0, graph_mode=False, unconstrained=False, model=\"exp\"\n",
    "    ):\n",
    "        \"\"\"Explain a single node prediction\n",
    "        \"\"\"\n",
    "        # index of the query node in the new adj\n",
    "        if graph_mode:\n",
    "            node_idx_new = node_idx\n",
    "            sub_adj = self.adj[graph_idx]\n",
    "            sub_feat = self.feat[graph_idx, :]\n",
    "            sub_label = self.label[graph_idx]\n",
    "            neighbors = np.asarray(range(self.adj.shape[0]))\n",
    "        else:\n",
    "            print(\"The node label :\", self.label[graph_idx][node_idx])\n",
    "            node_idx_new, sub_adj, sub_feat, sub_label, neighbors = self.extract_neighborhood(\n",
    "                node_idx, graph_idx\n",
    "            )\n",
    "            print(\"Neighbouring graph index for node \" + str(node_idx) + \" with new node index \" + str(node_idx_new))\n",
    "            sub_label = np.expand_dims(sub_label, axis=0)\n",
    "        \n",
    "        sub_adj = np.expand_dims(sub_adj, axis=0)\n",
    "        sub_feat = np.expand_dims(sub_feat, axis=0)\n",
    "\n",
    "        adj   = torch.tensor(sub_adj, dtype=torch.float)\n",
    "        x     = torch.tensor(sub_feat, requires_grad=True, dtype=torch.float)\n",
    "        label = torch.tensor(sub_label, dtype=torch.long)\n",
    "\n",
    "        if self.graph_mode:\n",
    "            pred_label = np.argmax(self.pred[0][graph_idx], axis=0)\n",
    "            print(\"Graph predicted label: \", pred_label)\n",
    "        else:\n",
    "            pred_label = np.argmax(self.pred[graph_idx][neighbors], axis=1)\n",
    "            print(\"Neighbours of predicted node labels :\", self.pred[graph_idx][neighbors])\n",
    "            print(\"Argmax of predicted node label:\", pred_label)\n",
    "            print(\"Predicted node label: \", pred_label[node_idx_new])\n",
    "\n",
    "        explainer = ExplainModule(\n",
    "            adj=adj,\n",
    "            x=x,\n",
    "            model=self.model,\n",
    "            label=label,\n",
    "            args=self.args,\n",
    "            writer=self.writer,\n",
    "            graph_idx=self.graph_idx,\n",
    "            graph_mode=self.graph_mode,\n",
    "        )\n",
    "        if self.args.gpu:\n",
    "            explainer = explainer.cuda()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # gradient baseline\n",
    "        if model == \"grad\":\n",
    "            explainer.zero_grad()\n",
    "            # pdb.set_trace()\n",
    "            adj_grad = torch.abs(\n",
    "                explainer.adj_feat_grad(node_idx_new, pred_label[node_idx_new])[0]\n",
    "            )[graph_idx]\n",
    "            masked_adj = adj_grad + adj_grad.t()\n",
    "            masked_adj = nn.functional.sigmoid(masked_adj)\n",
    "            masked_adj = masked_adj.cpu().detach().numpy() * sub_adj.squeeze()\n",
    "        else:\n",
    "            explainer.train()\n",
    "            begin_time = time.time()\n",
    "            \n",
    "            for epoch in range(self.args.num_epochs):\n",
    "                explainer.zero_grad()\n",
    "                explainer.optimizer.zero_grad()\n",
    "                ypred, adj_atts = explainer(node_idx_new, unconstrained=unconstrained)\n",
    "                \n",
    "                print(\"ypred for one epoch :\", ypred)\n",
    "\n",
    "                loss = explainer.loss(ypred, pred_label, node_idx_new, epoch)                \n",
    "                loss.backward()\n",
    "\n",
    "                explainer.optimizer.step()\n",
    "                if explainer.scheduler is not None:\n",
    "                    explainer.scheduler.step()\n",
    "\n",
    "                mask_density = explainer.mask_density()\n",
    "                if self.print_training:\n",
    "                    print(\n",
    "                        \"epoch: \",\n",
    "                        epoch,\n",
    "                        \"; loss: \",\n",
    "                        loss.item(),\n",
    "                        \"; mask density: \",\n",
    "                        mask_density.item(),\n",
    "                        \"; pred: \",\n",
    "                        ypred,\n",
    "                    )\n",
    "                    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "                single_subgraph_label = sub_label.squeeze()\n",
    "\n",
    "                if self.writer is not None:\n",
    "                    self.writer.add_scalar(\"mask/density\", mask_density, epoch)\n",
    "                    self.writer.add_scalar(\n",
    "                        \"optimization/lr\",\n",
    "                        explainer.optimizer.param_groups[0][\"lr\"],\n",
    "                        epoch,\n",
    "                    )\n",
    "                    if epoch % 25 == 0:\n",
    "                        explainer.log_mask(epoch)\n",
    "                        explainer.log_masked_adj(\n",
    "                            node_idx_new, epoch, label=single_subgraph_label\n",
    "                        )\n",
    "                        explainer.log_adj_grad(\n",
    "                            node_idx_new, pred_label, epoch, label=single_subgraph_label\n",
    "                        )\n",
    "\n",
    "                    if epoch == 0:\n",
    "                        if self.model.att:\n",
    "                            # explain node\n",
    "                            print(\"adj att size: \", adj_atts.size())\n",
    "                            adj_att = torch.sum(adj_atts[0], dim=2)\n",
    "                            # adj_att = adj_att[neighbors][:, neighbors]\n",
    "                            node_adj_att = adj_att * adj.float().cuda()\n",
    "                            io_utils.log_matrix(\n",
    "                                self.writer, node_adj_att[0], \"att/matrix\", epoch\n",
    "                            )\n",
    "                            node_adj_att = node_adj_att[0].cpu().detach().numpy()\n",
    "                            G = io_utils.denoise_graph(\n",
    "                                node_adj_att,\n",
    "                                node_idx_new,\n",
    "                                threshold=3.8,  # threshold_num=20,\n",
    "                                max_component=True,\n",
    "                            )\n",
    "                            io_utils.log_graph(\n",
    "                                self.writer,\n",
    "                                G,\n",
    "                                name=\"att/graph\",\n",
    "                                identify_self=not self.graph_mode,\n",
    "                                nodecolor=\"label\",\n",
    "                                edge_vmax=None,\n",
    "                                args=self.args,\n",
    "                            )\n",
    "                if model != \"exp\":\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            print(\"\\n----------------------\")\n",
    "            print(\"ypred : \", ypred)\n",
    "            print(\"pred_label : \", pred_label)\n",
    "            print(\"node_idx_new : \", node_idx_new)\n",
    "            \n",
    "            print(\"Completed training in \", time.time() - begin_time)\n",
    "            if model == \"exp\":\n",
    "                masked_adj = (\n",
    "                    explainer.masked_adj[0].cpu().detach().numpy() * sub_adj.squeeze()\n",
    "                )\n",
    "            else:\n",
    "                adj_atts = nn.functional.sigmoid(adj_atts).squeeze()\n",
    "                masked_adj = adj_atts.cpu().detach().numpy() * sub_adj.squeeze()\n",
    "\n",
    "        # io_utils.gen_explainer_prefix\n",
    "        fname = 'masked_adj_' + gen_explainer_prefix(self.args) + (\n",
    "                '_node_idx_'+str(node_idx)+'_graph_idx_'+str(self.graph_idx)+'.npy')\n",
    "        with open(os.path.join(self.args.logdir, fname), 'wb') as outfile:\n",
    "            np.save(outfile, np.asarray(masked_adj.copy()))\n",
    "            print(\"Saved adjacency matrix to \\\"\" + fname + \"\\\".\")\n",
    "        return masked_adj\n",
    "\n",
    "    # Utilities\n",
    "    def extract_neighborhood(self, node_idx, graph_idx=0):\n",
    "        \"\"\"Returns the neighborhood of a given ndoe.\"\"\"\n",
    "        neighbors_adj_row = self.neighborhoods[graph_idx][node_idx, :]\n",
    "        # index of the query node in the new adj\n",
    "        node_idx_new = sum(neighbors_adj_row[:node_idx])\n",
    "        neighbors = np.nonzero(neighbors_adj_row)[0]\n",
    "        sub_adj = self.adj[graph_idx][neighbors][:, neighbors]\n",
    "        sub_feat = self.feat[graph_idx, neighbors]\n",
    "        sub_label = self.label[graph_idx][neighbors]\n",
    "        return node_idx_new, sub_adj, sub_feat, sub_label, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.graph_utils as graph_utils\n",
    "\"\"\"\n",
    "Returns the n_hops degree adjacency matrix adj.\n",
    "\"\"\"\n",
    "def get_neighborhoods(adj, n_hops, use_cuda):\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    if use_cuda:\n",
    "        adj = adj.cuda()\n",
    "    hop_adj = power_adj = adj\n",
    "    for i in range(n_hops - 1):\n",
    "        power_adj = power_adj @ adj\n",
    "        prev_hop_adj = hop_adj\n",
    "        hop_adj = hop_adj + power_adj\n",
    "        hop_adj = (hop_adj > 0).float()\n",
    "    return hop_adj.cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_args.explain_node = 25\n",
    "graph_idx=0\n",
    "node_idx = prog_args.explain_node\n",
    "label = cg_dict[\"label\"]\n",
    "label[graph_idx][node_idx]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to explain node : 25\n",
      "Shape of neighborhoods : (35,)\n",
      "List of neighborhoods :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hops = prog_args.num_gc_layers # 3\n",
    "adj = cg_dict[\"adj\"]\n",
    "neighborhoods = get_neighborhoods(adj=adj, n_hops=n_hops, use_cuda=False)\n",
    "print(\"Attempt to explain node :\", node_idx)\n",
    "print(\"Shape of neighborhoods :\", neighborhoods[graph_idx][node_idx].shape)\n",
    "print(\"List of neighborhoods :\")\n",
    "neighborhoods[graph_idx][node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of neighborhoods adj : (25,)\n",
      "List of neighborhoods up to node index : 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors_adj_row = neighborhoods[graph_idx][node_idx, :]\n",
    "\n",
    "print(\"Shape of neighborhoods adj :\", neighbors_adj_row[:node_idx].shape)\n",
    "print(\"List of neighborhoods up to node index :\", node_idx)\n",
    "neighbors_adj_row[:node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_idx_new = sum(neighbors_adj_row[:node_idx])\n",
    "node_idx_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth, node label:  1\n",
      "node_idx : 25\n",
      "graph_idx : 0\n",
      "Returning new node index : 20\n",
      "Neighbouring graph index for node 25 with new node index 20\n",
      "Expand dimension of Subgraph label :\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3]]\n",
      "Subgraph neighbors :\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 25 26 27 28\n",
      " 29]\n",
      "Predicted labels :\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3]\n",
      "Predicted node label:  1\n"
     ]
    }
   ],
   "source": [
    "feat = cg_dict[\"feat\"]\n",
    "pred = cg_dict[\"pred\"]\n",
    "\n",
    "# Need to remove self or it will be initialize with zero values of the variables due to __init__\n",
    "def extract_neighborhood(node_idx, graph_idx=0):\n",
    "    \"\"\"Returns the neighborhood of a given node.\"\"\"\n",
    "    print(\"node_idx :\", node_idx)\n",
    "    print(\"graph_idx :\", graph_idx)\n",
    "\n",
    "    neighbors_adj_row = neighborhoods[graph_idx][node_idx, :]\n",
    "    # index of the query node in the new adj\n",
    "    node_idx_new = sum(neighbors_adj_row[:node_idx])\n",
    "    neighbors = np.nonzero(neighbors_adj_row)[0]\n",
    "    sub_adj = adj[graph_idx][neighbors][:, neighbors]\n",
    "    sub_feat = feat[graph_idx, neighbors]\n",
    "    sub_label = label[graph_idx][neighbors]\n",
    "    print(\"Returning new node index :\", node_idx_new)\n",
    "    return node_idx_new, sub_adj, sub_feat, sub_label, neighbors\n",
    "\n",
    "print(\"Ground truth, node label: \", label[graph_idx][node_idx])\n",
    "\n",
    "# Computational graph by extracting the neighbors\n",
    "node_idx_new, sub_adj, sub_feat, sub_label, neighbors = extract_neighborhood(node_idx, graph_idx)\n",
    "\n",
    "sub_adj = np.expand_dims(sub_adj, axis=0)\n",
    "sub_feat = np.expand_dims(sub_feat, axis=0)\n",
    "sub_label = np.expand_dims(sub_label, axis=0)\n",
    "\n",
    "print(\"Neighbouring graph index for node \" + str(node_idx) + \" with new node index \" + str(node_idx_new))\n",
    "#print(\"Expand dimension of Subgraph adjacency :\\n\", sub_adj)\n",
    "#print(\"Expand dimension of Subgraph features :\\n\", sub_feat)\n",
    "print(\"Expand dimension of Subgraph label :\\n\", sub_label)\n",
    "\n",
    "# All the nodes in the graph (eg. indexes from 0 to 34)\n",
    "print(\"Subgraph neighbors :\\n\", neighbors)\n",
    "\n",
    "tensor_adj   = torch.tensor(sub_adj, dtype=torch.float)\n",
    "tensor_x     = torch.tensor(sub_feat, requires_grad=True, dtype=torch.float)\n",
    "tensor_label = torch.tensor(sub_label, dtype=torch.long)\n",
    "\n",
    "pred_label = np.argmax(pred[graph_idx][neighbors], axis=1)\n",
    "print(\"Predicted labels :\\n\", pred_label)\n",
    "print(\"Predicted node label: \", pred_label[node_idx_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class ExplainModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adj,\n",
    "        x,\n",
    "        model,\n",
    "        label,\n",
    "        args,\n",
    "        graph_idx=0,\n",
    "        writer=None,\n",
    "        use_sigmoid=True,\n",
    "        graph_mode=False,\n",
    "    ):\n",
    "        super(ExplainModule, self).__init__()\n",
    "        self.adj = adj\n",
    "        self.x = x\n",
    "        self.model = model\n",
    "        self.label = label\n",
    "        self.graph_idx = graph_idx\n",
    "        self.args = args\n",
    "        self.writer = writer\n",
    "        self.mask_act = args.mask_act\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.graph_mode = graph_mode\n",
    "\n",
    "        init_strategy = \"normal\"\n",
    "        num_nodes = adj.size()[1]\n",
    "        \n",
    "        print(\"Explain module for {} nodes.\".format(num_nodes))\n",
    "        print(\"Explain module with model :\\n\", model)\n",
    "\n",
    "        self.mask, self.mask_bias = self.construct_edge_mask(\n",
    "            num_nodes, init_strategy=init_strategy\n",
    "        )\n",
    "\n",
    "        self.feat_mask = self.construct_feat_mask(x.size(-1), init_strategy=\"constant\")\n",
    "        params = [self.mask, self.feat_mask]\n",
    "        if self.mask_bias is not None:\n",
    "            params.append(self.mask_bias)\n",
    "        # For masking diagonal entries\n",
    "        self.diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes)\n",
    "        if args.gpu:\n",
    "            self.diag_mask = self.diag_mask.cuda()\n",
    "\n",
    "        # train_utils.build_optimizer\n",
    "        #self.scheduler, self.optimizer = build_optimizer(args, params)\n",
    "        \n",
    "        # Insert for optimizer and scheduler\n",
    "        filter_fn = filter(lambda p : p.requires_grad, model.parameters())\n",
    "        self.optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=0.0)\n",
    "        self.scheduler = None\n",
    "\n",
    "        self.coeffs = {\n",
    "            \"size\": 0.005,\n",
    "            \"feat_size\": 1.0,\n",
    "            \"ent\": 1.0,\n",
    "            \"feat_ent\": 0.1,\n",
    "            \"grad\": 0,\n",
    "            \"lap\": 1.0,\n",
    "        }\n",
    "\n",
    "    def construct_feat_mask(self, feat_dim, init_strategy=\"normal\"):\n",
    "        mask = nn.Parameter(torch.FloatTensor(feat_dim))\n",
    "        if init_strategy == \"normal\":\n",
    "            std = 0.1\n",
    "            with torch.no_grad():\n",
    "                mask.normal_(1.0, std)\n",
    "        elif init_strategy == \"constant\":\n",
    "            with torch.no_grad():\n",
    "                nn.init.constant_(mask, 0.0)\n",
    "                # mask[0] = 2\n",
    "        return mask\n",
    "\n",
    "    def construct_edge_mask(self, num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
    "        mask = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
    "        if init_strategy == \"normal\":\n",
    "            std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "                2.0 / (num_nodes + num_nodes)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                mask.normal_(1.0, std)\n",
    "                # mask.clamp_(0.0, 1.0)\n",
    "        elif init_strategy == \"const\":\n",
    "            nn.init.constant_(mask, const_val)\n",
    "\n",
    "        if self.args.mask_bias:\n",
    "            mask_bias = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
    "            nn.init.constant_(mask_bias, 0.0)\n",
    "        else:\n",
    "            mask_bias = None\n",
    "\n",
    "        return mask, mask_bias\n",
    "\n",
    "    def _masked_adj(self):\n",
    "        sym_mask = self.mask\n",
    "        if self.mask_act == \"sigmoid\":\n",
    "            sym_mask = torch.sigmoid(self.mask)\n",
    "        elif self.mask_act == \"ReLU\":\n",
    "            sym_mask = nn.ReLU()(self.mask)\n",
    "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "        adj = self.adj.cuda() if self.args.gpu else self.adj\n",
    "        masked_adj = adj * sym_mask\n",
    "        if self.args.mask_bias:\n",
    "            bias = (self.mask_bias + self.mask_bias.t()) / 2\n",
    "            bias = nn.ReLU6()(bias * 6) / 6\n",
    "            masked_adj += (bias + bias.t()) / 2\n",
    "        return masked_adj * self.diag_mask\n",
    "\n",
    "    def mask_density(self):\n",
    "        mask_sum = torch.sum(self._masked_adj()).cpu()\n",
    "        adj_sum = torch.sum(self.adj)\n",
    "        return mask_sum / adj_sum\n",
    "\n",
    "    def forward(self, node_idx, unconstrained=False, mask_features=True, marginalize=False):\n",
    "        print(\"Started Training ... for :\", node_idx)\n",
    "        \n",
    "        x = self.x.cuda() if self.args.gpu else self.x\n",
    "\n",
    "        if unconstrained:\n",
    "            sym_mask = torch.sigmoid(self.mask) if self.use_sigmoid else self.mask\n",
    "            self.masked_adj = (\n",
    "                torch.unsqueeze((sym_mask + sym_mask.t()) / 2, 0) * self.diag_mask\n",
    "            )\n",
    "        else:\n",
    "            self.masked_adj = self._masked_adj()\n",
    "            if mask_features:\n",
    "                feat_mask = (\n",
    "                    torch.sigmoid(self.feat_mask)\n",
    "                    if self.use_sigmoid\n",
    "                    else self.feat_mask\n",
    "                )\n",
    "                if marginalize:\n",
    "                    std_tensor = torch.ones_like(x, dtype=torch.float) / 2\n",
    "                    mean_tensor = torch.zeros_like(x, dtype=torch.float) - x\n",
    "                    z = torch.normal(mean=mean_tensor, std=std_tensor)\n",
    "                    x = x + z * (1 - feat_mask)\n",
    "                else:\n",
    "                    x = x * feat_mask\n",
    "\n",
    "        ypred, adj_att = self.model(x, self.masked_adj)\n",
    "        if self.graph_mode:\n",
    "            res = nn.Softmax(dim=0)(ypred[0])\n",
    "        else:\n",
    "            node_pred = ypred[self.graph_idx, node_idx, :]\n",
    "            res = nn.Softmax(dim=0)(node_pred)\n",
    "            \n",
    "        print(\"Node prediction with softmax after one epoch :\", res)\n",
    "        return res, adj_att\n",
    "\n",
    "    def adj_feat_grad(self, node_idx, pred_label_node):\n",
    "        self.model.zero_grad()\n",
    "        self.adj.requires_grad = True\n",
    "        self.x.requires_grad = True\n",
    "        if self.adj.grad is not None:\n",
    "            self.adj.grad.zero_()\n",
    "            self.x.grad.zero_()\n",
    "        if self.args.gpu:\n",
    "            adj = self.adj.cuda()\n",
    "            x = self.x.cuda()\n",
    "            label = self.label.cuda()\n",
    "        else:\n",
    "            x, adj = self.x, self.adj\n",
    "        ypred, _ = self.model(x, adj)\n",
    "        if self.graph_mode:\n",
    "            logit = nn.Softmax(dim=0)(ypred[0])\n",
    "        else:\n",
    "            logit = nn.Softmax(dim=0)(ypred[self.graph_idx, node_idx, :])\n",
    "        logit = logit[pred_label_node]\n",
    "        loss = -torch.log(logit)\n",
    "        loss.backward()\n",
    "        return self.adj.grad, self.x.grad\n",
    "\n",
    "    def loss(self, pred, pred_label, node_idx, epoch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: prediction made by current model\n",
    "            pred_label: the label predicted by the original model.\n",
    "        \"\"\"\n",
    "        mi_obj = False\n",
    "        if mi_obj:\n",
    "            pred_loss = -torch.sum(pred * torch.log(pred))\n",
    "        else:\n",
    "            pred_label_node = pred_label if self.graph_mode else pred_label[node_idx]\n",
    "            gt_label_node = self.label if self.graph_mode else self.label[0][node_idx]\n",
    "            logit = pred[gt_label_node]\n",
    "            pred_loss = -torch.log(logit)\n",
    "        # size\n",
    "        mask = self.mask\n",
    "        if self.mask_act == \"sigmoid\":\n",
    "            mask = torch.sigmoid(self.mask)\n",
    "        elif self.mask_act == \"ReLU\":\n",
    "            mask = nn.ReLU()(self.mask)\n",
    "        size_loss = self.coeffs[\"size\"] * torch.sum(mask)\n",
    "\n",
    "        # pre_mask_sum = torch.sum(self.feat_mask)\n",
    "        feat_mask = (\n",
    "            torch.sigmoid(self.feat_mask) if self.use_sigmoid else self.feat_mask\n",
    "        )\n",
    "        feat_size_loss = self.coeffs[\"feat_size\"] * torch.mean(feat_mask)\n",
    "\n",
    "        # entropy\n",
    "        mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
    "        mask_ent_loss = self.coeffs[\"ent\"] * torch.mean(mask_ent)\n",
    "\n",
    "        feat_mask_ent = - feat_mask             \\\n",
    "                        * torch.log(feat_mask)  \\\n",
    "                        - (1 - feat_mask)       \\\n",
    "                        * torch.log(1 - feat_mask)\n",
    "\n",
    "        feat_mask_ent_loss = self.coeffs[\"feat_ent\"] * torch.mean(feat_mask_ent)\n",
    "\n",
    "        # laplacian\n",
    "        D = torch.diag(torch.sum(self.masked_adj[0], 0))\n",
    "        m_adj = self.masked_adj if self.graph_mode else self.masked_adj[self.graph_idx]\n",
    "        L = D - m_adj\n",
    "        pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n",
    "        if self.args.gpu:\n",
    "            pred_label_t = pred_label_t.cuda()\n",
    "            L = L.cuda()\n",
    "        if self.graph_mode:\n",
    "            lap_loss = 0\n",
    "        else:\n",
    "            lap_loss = (self.coeffs[\"lap\"]\n",
    "                * (pred_label_t @ L @ pred_label_t)\n",
    "                / self.adj.numel()\n",
    "            )\n",
    "\n",
    "        # grad\n",
    "        # adj\n",
    "        # adj_grad, x_grad = self.adj_feat_grad(node_idx, pred_label_node)\n",
    "        # adj_grad = adj_grad[self.graph_idx]\n",
    "        # x_grad = x_grad[self.graph_idx]\n",
    "        # if self.args.gpu:\n",
    "        #    adj_grad = adj_grad.cuda()\n",
    "        # grad_loss = self.coeffs['grad'] * -torch.mean(torch.abs(adj_grad) * mask)\n",
    "\n",
    "        # feat\n",
    "        # x_grad_sum = torch.sum(x_grad, 1)\n",
    "        # grad_feat_loss = self.coeffs['featgrad'] * -torch.mean(x_grad_sum * mask)\n",
    "\n",
    "        loss = pred_loss + size_loss + lap_loss + mask_ent_loss + feat_size_loss\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(\"optimization/size_loss\", size_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
    "            self.writer.add_scalar(\n",
    "                \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
    "            )\n",
    "            # self.writer.add_scalar('optimization/grad_loss', grad_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/pred_loss\", pred_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/lap_loss\", lap_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/overall_loss\", loss, epoch)\n",
    "        return loss\n",
    "\n",
    "    def log_mask(self, epoch):\n",
    "        plt.switch_backend(\"agg\")\n",
    "        fig = plt.figure(figsize=(4, 3), dpi=400)\n",
    "        plt.imshow(self.mask.cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        self.writer.add_image(\n",
    "            \"mask/mask\", tensorboardX.utils.figure_to_image(fig), epoch\n",
    "        )\n",
    "\n",
    "        # fig = plt.figure(figsize=(4,3), dpi=400)\n",
    "        # plt.imshow(self.feat_mask.cpu().detach().numpy()[:,np.newaxis], cmap=plt.get_cmap('BuPu'))\n",
    "        # cbar = plt.colorbar()\n",
    "        # cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # fig.canvas.draw()\n",
    "        # self.writer.add_image('mask/feat_mask', tensorboardX.utils.figure_to_image(fig), epoch)\n",
    "        io_utils.log_matrix(\n",
    "            self.writer, torch.sigmoid(self.feat_mask), \"mask/feat_mask\", epoch\n",
    "        )\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 3), dpi=400)\n",
    "        # use [0] to remove the batch dim\n",
    "        plt.imshow(self.masked_adj[0].cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        self.writer.add_image(\n",
    "            \"mask/adj\", tensorboardX.utils.figure_to_image(fig), epoch\n",
    "        )\n",
    "\n",
    "        if self.args.mask_bias:\n",
    "            fig = plt.figure(figsize=(4, 3), dpi=400)\n",
    "            # use [0] to remove the batch dim\n",
    "            plt.imshow(self.mask_bias.cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.canvas.draw()\n",
    "            self.writer.add_image(\n",
    "                \"mask/bias\", tensorboardX.utils.figure_to_image(fig), epoch\n",
    "            )\n",
    "\n",
    "    def log_adj_grad(self, node_idx, pred_label, epoch, label=None):\n",
    "        log_adj = False\n",
    "\n",
    "        if self.graph_mode:\n",
    "            predicted_label = pred_label\n",
    "            # adj_grad, x_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[0]\n",
    "            adj_grad, x_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
    "            adj_grad = torch.abs(adj_grad)[0]\n",
    "            x_grad = torch.sum(x_grad[0], 0, keepdim=True).t()\n",
    "        else:\n",
    "            predicted_label = pred_label[node_idx]\n",
    "            # adj_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[self.graph_idx]\n",
    "            adj_grad, x_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
    "            adj_grad = torch.abs(adj_grad)[self.graph_idx]\n",
    "            x_grad = x_grad[self.graph_idx][node_idx][:, np.newaxis]\n",
    "            # x_grad = torch.sum(x_grad[self.graph_idx], 0, keepdim=True).t()\n",
    "        adj_grad = (adj_grad + adj_grad.t()) / 2\n",
    "        adj_grad = (adj_grad * self.adj).squeeze()\n",
    "        if log_adj:\n",
    "            io_utils.log_matrix(self.writer, adj_grad, \"grad/adj_masked\", epoch)\n",
    "            self.adj.requires_grad = False\n",
    "            io_utils.log_matrix(self.writer, self.adj.squeeze(), \"grad/adj_orig\", epoch)\n",
    "\n",
    "        masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
    "\n",
    "        # only for graph mode since many node neighborhoods for syn tasks are relatively large for\n",
    "        # visualization\n",
    "        if self.graph_mode:\n",
    "            G = io_utils.denoise_graph(\n",
    "                masked_adj, node_idx, feat=self.x[0], threshold=None, max_component=False\n",
    "            )\n",
    "            io_utils.log_graph(\n",
    "                self.writer,\n",
    "                G,\n",
    "                name=\"grad/graph_orig\",\n",
    "                epoch=epoch,\n",
    "                identify_self=False,\n",
    "                label_node_feat=True,\n",
    "                nodecolor=\"feat\",\n",
    "                edge_vmax=None,\n",
    "                args=self.args,\n",
    "            )\n",
    "        io_utils.log_matrix(self.writer, x_grad, \"grad/feat\", epoch)\n",
    "\n",
    "        adj_grad = adj_grad.detach().numpy()\n",
    "        if self.graph_mode:\n",
    "            print(\"GRAPH model\")\n",
    "            G = io_utils.denoise_graph(\n",
    "                adj_grad,\n",
    "                node_idx,\n",
    "                feat=self.x[0],\n",
    "                threshold=0.0003,  # threshold_num=20,\n",
    "                max_component=True,\n",
    "            )\n",
    "            io_utils.log_graph(\n",
    "                self.writer,\n",
    "                G,\n",
    "                name=\"grad/graph\",\n",
    "                epoch=epoch,\n",
    "                identify_self=False,\n",
    "                label_node_feat=True,\n",
    "                nodecolor=\"feat\",\n",
    "                edge_vmax=None,\n",
    "                args=self.args,\n",
    "            )\n",
    "        else:\n",
    "            # G = io_utils.denoise_graph(adj_grad, node_idx, label=label, threshold=0.5)\n",
    "            G = io_utils.denoise_graph(adj_grad, node_idx, threshold_num=12)\n",
    "            io_utils.log_graph(\n",
    "                self.writer, G, name=\"grad/graph\", epoch=epoch, args=self.args\n",
    "            )\n",
    "\n",
    "        # if graph attention, also visualize att\n",
    "\n",
    "    def log_masked_adj(self, node_idx, epoch, name=\"mask/graph\", label=None):\n",
    "        # use [0] to remove the batch dim\n",
    "        masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
    "        if self.graph_mode:\n",
    "            G = io_utils.denoise_graph(\n",
    "                masked_adj,\n",
    "                node_idx,\n",
    "                feat=self.x[0],\n",
    "                threshold=0.2,  # threshold_num=20,\n",
    "                max_component=True,\n",
    "            )\n",
    "            io_utils.log_graph(\n",
    "                self.writer,\n",
    "                G,\n",
    "                name=name,\n",
    "                identify_self=False,\n",
    "                nodecolor=\"feat\",\n",
    "                epoch=epoch,\n",
    "                label_node_feat=True,\n",
    "                edge_vmax=None,\n",
    "                args=self.args,\n",
    "            )\n",
    "        else:\n",
    "            G = io_utils.denoise_graph(\n",
    "                masked_adj, node_idx, threshold_num=12, max_component=True\n",
    "            )\n",
    "            io_utils.log_graph(\n",
    "                self.writer,\n",
    "                G,\n",
    "                name=name,\n",
    "                identify_self=True,\n",
    "                nodecolor=\"label\",\n",
    "                epoch=epoch,\n",
    "                edge_vmax=None,\n",
    "                args=self.args,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain module for 25 nodes.\n",
      "Explain module with model :\n",
      " GcnEncoderNode(\n",
      "  (conv_first): GraphConv()\n",
      "  (conv_block): ModuleList(\n",
      "    (0): GraphConv()\n",
      "  )\n",
      "  (conv_last): GraphConv()\n",
      "  (act): ReLU()\n",
      "  (pred_model): Linear(in_features=60, out_features=4, bias=True)\n",
      "  (celoss): CrossEntropyLoss()\n",
      ")\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0338, 0.1452, 0.7924, 0.0285], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  0 ; loss:  5.292957305908203 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0338, 0.1452, 0.7924, 0.0285], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0360, 0.2023, 0.7409, 0.0208], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  1 ; loss:  4.961694240570068 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0360, 0.2023, 0.7409, 0.0208], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0416, 0.2933, 0.6490, 0.0161], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  2 ; loss:  4.590244293212891 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0416, 0.2933, 0.6490, 0.0161], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0506, 0.4286, 0.5077, 0.0131], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  3 ; loss:  4.2109222412109375 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0506, 0.4286, 0.5077, 0.0131], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0604, 0.5898, 0.3389, 0.0110], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  4 ; loss:  3.8916873931884766 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0604, 0.5898, 0.3389, 0.0110], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0685, 0.7195, 0.2024, 0.0096], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  5 ; loss:  3.6928722858428955 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0685, 0.7195, 0.2024, 0.0096], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0753, 0.7949, 0.1208, 0.0090], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  6 ; loss:  3.593191623687744 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0753, 0.7949, 0.1208, 0.0090], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0811, 0.8323, 0.0778, 0.0088], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  7 ; loss:  3.5472147464752197 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0811, 0.8323, 0.0778, 0.0088], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0855, 0.8506, 0.0551, 0.0089], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  8 ; loss:  3.525526762008667 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0855, 0.8506, 0.0551, 0.0089], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0874, 0.8609, 0.0426, 0.0090], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  9 ; loss:  3.5134494304656982 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0874, 0.8609, 0.0426, 0.0090], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0870, 0.8685, 0.0352, 0.0092], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  10 ; loss:  3.5046136379241943 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0870, 0.8685, 0.0352, 0.0092], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0858, 0.8743, 0.0305, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  11 ; loss:  3.4979324340820312 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0858, 0.8743, 0.0305, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0845, 0.8787, 0.0273, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  12 ; loss:  3.492934226989746 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0845, 0.8787, 0.0273, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0827, 0.8828, 0.0250, 0.0095], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  13 ; loss:  3.4882843494415283 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0827, 0.8828, 0.0250, 0.0095], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0806, 0.8867, 0.0233, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  14 ; loss:  3.4839508533477783 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0806, 0.8867, 0.0233, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0786, 0.8901, 0.0219, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  15 ; loss:  3.4800684452056885 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0786, 0.8901, 0.0219, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0761, 0.8936, 0.0209, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  16 ; loss:  3.4761180877685547 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0761, 0.8936, 0.0209, 0.0094], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0735, 0.8972, 0.0201, 0.0093], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  17 ; loss:  3.4721553325653076 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0735, 0.8972, 0.0201, 0.0093], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0707, 0.9007, 0.0195, 0.0092], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  18 ; loss:  3.4682419300079346 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0707, 0.9007, 0.0195, 0.0092], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0680, 0.9040, 0.0190, 0.0091], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  19 ; loss:  3.464595079421997 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0680, 0.9040, 0.0190, 0.0091], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0653, 0.9072, 0.0186, 0.0089], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  20 ; loss:  3.4610707759857178 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0653, 0.9072, 0.0186, 0.0089], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0626, 0.9103, 0.0183, 0.0088], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  21 ; loss:  3.4576456546783447 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0626, 0.9103, 0.0183, 0.0088], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0599, 0.9134, 0.0181, 0.0087], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  22 ; loss:  3.4542770385742188 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0599, 0.9134, 0.0181, 0.0087], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0571, 0.9164, 0.0180, 0.0085], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  23 ; loss:  3.451007843017578 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0571, 0.9164, 0.0180, 0.0085], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0545, 0.9192, 0.0179, 0.0084], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  24 ; loss:  3.447892904281616 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0545, 0.9192, 0.0179, 0.0084], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0520, 0.9219, 0.0178, 0.0082], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  25 ; loss:  3.444944143295288 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0520, 0.9219, 0.0178, 0.0082], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0498, 0.9244, 0.0177, 0.0081], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  26 ; loss:  3.4422953128814697 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0498, 0.9244, 0.0177, 0.0081], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0477, 0.9267, 0.0176, 0.0080], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  27 ; loss:  3.439809560775757 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0477, 0.9267, 0.0176, 0.0080], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0457, 0.9288, 0.0176, 0.0078], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  28 ; loss:  3.4374778270721436 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0457, 0.9288, 0.0176, 0.0078], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0439, 0.9309, 0.0175, 0.0077], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  29 ; loss:  3.435290813446045 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0439, 0.9309, 0.0175, 0.0077], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0422, 0.9328, 0.0175, 0.0076], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  30 ; loss:  3.4332387447357178 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0422, 0.9328, 0.0175, 0.0076], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0406, 0.9346, 0.0174, 0.0074], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  31 ; loss:  3.4313125610351562 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0406, 0.9346, 0.0174, 0.0074], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0390, 0.9363, 0.0174, 0.0073], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  32 ; loss:  3.4295029640197754 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0390, 0.9363, 0.0174, 0.0073], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0376, 0.9379, 0.0173, 0.0072], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  33 ; loss:  3.427799701690674 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0376, 0.9379, 0.0173, 0.0072], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0363, 0.9394, 0.0172, 0.0071], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  34 ; loss:  3.426194190979004 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0363, 0.9394, 0.0172, 0.0071], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0351, 0.9408, 0.0172, 0.0070], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  35 ; loss:  3.4246785640716553 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0351, 0.9408, 0.0172, 0.0070], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0339, 0.9422, 0.0171, 0.0068], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  36 ; loss:  3.4232327938079834 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0339, 0.9422, 0.0171, 0.0068], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0328, 0.9435, 0.0170, 0.0067], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  37 ; loss:  3.421860933303833 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0328, 0.9435, 0.0170, 0.0067], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0318, 0.9447, 0.0170, 0.0066], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  38 ; loss:  3.4205589294433594 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0318, 0.9447, 0.0170, 0.0066], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0308, 0.9459, 0.0169, 0.0065], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  39 ; loss:  3.419320821762085 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0308, 0.9459, 0.0169, 0.0065], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0299, 0.9470, 0.0168, 0.0064], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  40 ; loss:  3.4181416034698486 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0299, 0.9470, 0.0168, 0.0064], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0290, 0.9480, 0.0167, 0.0062], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  41 ; loss:  3.4170165061950684 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0290, 0.9480, 0.0167, 0.0062], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0282, 0.9491, 0.0166, 0.0061], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  42 ; loss:  3.415940523147583 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0282, 0.9491, 0.0166, 0.0061], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0275, 0.9500, 0.0165, 0.0060], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  43 ; loss:  3.4149105548858643 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0275, 0.9500, 0.0165, 0.0060], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0268, 0.9510, 0.0164, 0.0059], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  44 ; loss:  3.4139223098754883 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0268, 0.9510, 0.0164, 0.0059], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0261, 0.9519, 0.0162, 0.0058], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  45 ; loss:  3.412972927093506 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0261, 0.9519, 0.0162, 0.0058], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0255, 0.9528, 0.0161, 0.0057], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  46 ; loss:  3.4120593070983887 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0255, 0.9528, 0.0161, 0.0057], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0249, 0.9536, 0.0160, 0.0056], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  47 ; loss:  3.411179304122925 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0249, 0.9536, 0.0160, 0.0056], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0243, 0.9544, 0.0158, 0.0055], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  48 ; loss:  3.410330057144165 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0243, 0.9544, 0.0158, 0.0055], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0238, 0.9552, 0.0157, 0.0054], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  49 ; loss:  3.4095096588134766 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0238, 0.9552, 0.0157, 0.0054], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0233, 0.9559, 0.0155, 0.0053], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  50 ; loss:  3.4087162017822266 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0233, 0.9559, 0.0155, 0.0053], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0228, 0.9567, 0.0153, 0.0052], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  51 ; loss:  3.4079482555389404 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0228, 0.9567, 0.0153, 0.0052], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0223, 0.9574, 0.0152, 0.0051], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  52 ; loss:  3.4072039127349854 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0223, 0.9574, 0.0152, 0.0051], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0219, 0.9581, 0.0150, 0.0050], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  53 ; loss:  3.406482458114624 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0219, 0.9581, 0.0150, 0.0050], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0215, 0.9588, 0.0148, 0.0049], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  54 ; loss:  3.4057819843292236 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0215, 0.9588, 0.0148, 0.0049], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0211, 0.9594, 0.0146, 0.0049], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  55 ; loss:  3.405102252960205 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0211, 0.9594, 0.0146, 0.0049], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0207, 0.9600, 0.0145, 0.0048], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  56 ; loss:  3.4044415950775146 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0207, 0.9600, 0.0145, 0.0048], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0204, 0.9607, 0.0143, 0.0047], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  57 ; loss:  3.403799533843994 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0204, 0.9607, 0.0143, 0.0047], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0200, 0.9613, 0.0141, 0.0046], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  58 ; loss:  3.403175115585327 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0200, 0.9613, 0.0141, 0.0046], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0197, 0.9618, 0.0139, 0.0045], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  59 ; loss:  3.4025678634643555 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0197, 0.9618, 0.0139, 0.0045], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0194, 0.9624, 0.0137, 0.0044], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  60 ; loss:  3.4019768238067627 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0194, 0.9624, 0.0137, 0.0044], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0191, 0.9630, 0.0136, 0.0044], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  61 ; loss:  3.4014017581939697 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0191, 0.9630, 0.0136, 0.0044], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0188, 0.9635, 0.0134, 0.0043], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  62 ; loss:  3.40084171295166 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0188, 0.9635, 0.0134, 0.0043], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0186, 0.9640, 0.0132, 0.0042], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  63 ; loss:  3.400301218032837 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0186, 0.9640, 0.0132, 0.0042], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0183, 0.9645, 0.0130, 0.0042], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  64 ; loss:  3.3997745513916016 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0183, 0.9645, 0.0130, 0.0042], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0181, 0.9650, 0.0128, 0.0041], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  65 ; loss:  3.399256944656372 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0181, 0.9650, 0.0128, 0.0041], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0178, 0.9655, 0.0127, 0.0040], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  66 ; loss:  3.3987479209899902 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0178, 0.9655, 0.0127, 0.0040], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0176, 0.9660, 0.0125, 0.0039], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  67 ; loss:  3.3982510566711426 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0176, 0.9660, 0.0125, 0.0039], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0173, 0.9665, 0.0123, 0.0039], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  68 ; loss:  3.397766590118408 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0173, 0.9665, 0.0123, 0.0039], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0171, 0.9669, 0.0121, 0.0038], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  69 ; loss:  3.397294044494629 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0171, 0.9669, 0.0121, 0.0038], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0169, 0.9674, 0.0120, 0.0038], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  70 ; loss:  3.3968329429626465 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0169, 0.9674, 0.0120, 0.0038], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0167, 0.9678, 0.0118, 0.0037], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  71 ; loss:  3.396383047103882 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0167, 0.9678, 0.0118, 0.0037], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0165, 0.9682, 0.0116, 0.0036], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  72 ; loss:  3.395944356918335 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0165, 0.9682, 0.0116, 0.0036], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0163, 0.9686, 0.0115, 0.0036], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  73 ; loss:  3.3955161571502686 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0163, 0.9686, 0.0115, 0.0036], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0161, 0.9690, 0.0113, 0.0035], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  74 ; loss:  3.3950984477996826 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0161, 0.9690, 0.0113, 0.0035], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0159, 0.9694, 0.0112, 0.0035], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  75 ; loss:  3.394690990447998 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0159, 0.9694, 0.0112, 0.0035], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0157, 0.9698, 0.0110, 0.0034], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  76 ; loss:  3.3942933082580566 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0157, 0.9698, 0.0110, 0.0034], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0156, 0.9702, 0.0109, 0.0034], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  77 ; loss:  3.3939054012298584 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0156, 0.9702, 0.0109, 0.0034], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0154, 0.9706, 0.0107, 0.0033], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  78 ; loss:  3.393522262573242 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0154, 0.9706, 0.0107, 0.0033], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0152, 0.9709, 0.0106, 0.0032], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  79 ; loss:  3.3931446075439453 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0152, 0.9709, 0.0106, 0.0032], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0150, 0.9713, 0.0105, 0.0032], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  80 ; loss:  3.392775058746338 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0150, 0.9713, 0.0105, 0.0032], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0149, 0.9717, 0.0103, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  81 ; loss:  3.3924143314361572 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0149, 0.9717, 0.0103, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0147, 0.9720, 0.0102, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  82 ; loss:  3.392062187194824 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0147, 0.9720, 0.0102, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0145, 0.9723, 0.0101, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  83 ; loss:  3.391716241836548 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0145, 0.9723, 0.0101, 0.0031], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0144, 0.9727, 0.0100, 0.0030], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  84 ; loss:  3.3913779258728027 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0144, 0.9727, 0.0100, 0.0030], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node prediction with softmax after one epoch : tensor([0.0142, 0.9730, 0.0099, 0.0030], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  85 ; loss:  3.3910467624664307 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0142, 0.9730, 0.0099, 0.0030], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0140, 0.9733, 0.0098, 0.0029], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  86 ; loss:  3.39072322845459 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0140, 0.9733, 0.0098, 0.0029], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0139, 0.9736, 0.0096, 0.0029], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  87 ; loss:  3.390406847000122 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0139, 0.9736, 0.0096, 0.0029], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0137, 0.9739, 0.0095, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  88 ; loss:  3.3900973796844482 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0137, 0.9739, 0.0095, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0136, 0.9742, 0.0094, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  89 ; loss:  3.3897950649261475 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0136, 0.9742, 0.0094, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0134, 0.9745, 0.0093, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  90 ; loss:  3.3894989490509033 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0134, 0.9745, 0.0093, 0.0028], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0133, 0.9748, 0.0092, 0.0027], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  91 ; loss:  3.3892099857330322 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0133, 0.9748, 0.0092, 0.0027], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0131, 0.9750, 0.0092, 0.0027], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  92 ; loss:  3.3889269828796387 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0131, 0.9750, 0.0092, 0.0027], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0130, 0.9753, 0.0091, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  93 ; loss:  3.3886501789093018 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0130, 0.9753, 0.0091, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0128, 0.9756, 0.0090, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  94 ; loss:  3.3883795738220215 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0128, 0.9756, 0.0090, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0127, 0.9758, 0.0089, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  95 ; loss:  3.3881144523620605 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0127, 0.9758, 0.0089, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0126, 0.9761, 0.0088, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  96 ; loss:  3.387855291366577 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0126, 0.9761, 0.0088, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0124, 0.9763, 0.0087, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  97 ; loss:  3.387601613998413 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0124, 0.9763, 0.0087, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0123, 0.9766, 0.0086, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  98 ; loss:  3.3873534202575684 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0123, 0.9766, 0.0086, 0.0025], grad_fn=<SoftmaxBackward>)\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0122, 0.9768, 0.0086, 0.0024], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  99 ; loss:  3.387110471725464 ; mask density:  0.7274600863456726 ; pred:  tensor([0.0122, 0.9768, 0.0086, 0.0024], grad_fn=<SoftmaxBackward>)\n",
      "finished training in  0.7086033821105957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have to use the tensor version of adj for Tensor computation\n",
    "# \"--explain-node\", dest=\"explain_node\", type=int, help=\"Node to explain.\"\n",
    "# explain.py\n",
    "prog_args.explain_node = 20\n",
    "prog_args.mask_act=\"sigmoid\"\n",
    "prog_args.mask_bias = False\n",
    "prog_args.explainer_suffix=\"\"\n",
    "prog_args.num_epochs = 100\n",
    "unconstrained = False\n",
    "exp_model=\"exp\"\n",
    "\n",
    "explainerMod = ExplainModule(\n",
    "    adj=tensor_adj,                # adj\n",
    "    x=tensor_x,                    # x\n",
    "    model=model,                   # self.model\n",
    "    label=tensor_label,            # label\n",
    "    args=prog_args,                # self.args\n",
    "    writer=None,\n",
    "    graph_idx=graph_idx,           # self.graph_idx\n",
    "    graph_mode=graph_mode          # self.graph_mode\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "explainerMod.train()\n",
    "begin_time = time.time()\n",
    "\n",
    "for epoch in range(prog_args.num_epochs):\n",
    "    explainerMod.zero_grad()\n",
    "    explainerMod.optimizer.zero_grad()\n",
    "    \n",
    "    # node_idx_new is passed to explainerMod.forward to training with the new index\n",
    "    ypred, adj_atts = explainerMod(node_idx_new, unconstrained=unconstrained)\n",
    "    loss = explainerMod.loss(ypred, pred_label, node_idx_new, epoch)\n",
    "    loss.backward()\n",
    "    \n",
    "    explainerMod.optimizer.step()\n",
    "    mask_density = explainerMod.mask_density()\n",
    "    \n",
    "    print(\"epoch: \",\n",
    "          epoch,\n",
    "          \"; loss: \",\n",
    "          loss.item(),\n",
    "          \"; mask density: \",\n",
    "          mask_density.item(),\n",
    "          \"; pred: \",\n",
    "          ypred)\n",
    "    single_subgraph_label = sub_label.squeeze()\n",
    "\n",
    "    if exp_model != \"exp\":\n",
    "        break\n",
    "        \n",
    "print(\"finished training in \", time.time() - begin_time)\n",
    "\n",
    "if exp_model == \"exp\":\n",
    "    masked_adj = (\n",
    "        explainerMod.masked_adj[0].cpu().detach().numpy() * sub_adj.squeeze()\n",
    "    )\n",
    "\n",
    "# Trained mask adjacency matrix\n",
    "masked_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_args.explain_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn-model-explainer/explainer_main.py\n",
    "# Create explainer\n",
    "# from explainer import explain    \n",
    "# explain.Explainer\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    adj=cg_dict[\"adj\"],\n",
    "    feat=cg_dict[\"feat\"],\n",
    "    label=cg_dict[\"label\"],\n",
    "    pred=cg_dict[\"pred\"],\n",
    "    train_idx=cg_dict[\"train_idx\"],\n",
    "    args=prog_args,\n",
    "    writer=None, # writer\n",
    "    print_training=True,\n",
    "    graph_mode=graph_mode,\n",
    "    graph_idx=prog_args.graph_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn-model-explainer/utils/io_utils.py\n",
    "'''\n",
    "Generate label prefix for a graph explainer model.\n",
    "'''\n",
    "def gen_explainer_prefix(args):\n",
    "\n",
    "    name = gen_prefix(args) + \"_explain\"\n",
    "    if len(args.explainer_suffix) > 0:\n",
    "        name += \"_\" + args.explainer_suffix\n",
    "    \n",
    "    print(\"GNN Explainer prefix :\\\"\" + name + \"\\\".\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting single node label \n",
    "graph_idx = 0\n",
    "node_idx = 20\n",
    "cg_dict['label'][graph_idx][node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_args.explain_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The node label : 1\n",
      "Neighbouring graph index for node 20 with new node index 20\n",
      "Neighbours of predicted node labels : [[ 3.332701   -1.7189426  -5.147857   -2.3727663 ]\n",
      " [ 3.4665647  -1.9025738  -4.8109746  -2.5232244 ]\n",
      " [ 3.3718405  -2.1947322  -3.4699092  -2.49906   ]\n",
      " [ 3.4675326  -2.2590148  -3.7450867  -2.5032115 ]\n",
      " [ 3.520472   -2.097652   -4.4505296  -2.5113714 ]\n",
      " [ 3.3916693  -1.7525692  -4.9966416  -2.5122395 ]\n",
      " [ 3.3266418  -1.6766102  -5.1448884  -2.4393167 ]\n",
      " [ 3.4806757  -1.9337585  -4.7741647  -2.5245388 ]\n",
      " [ 3.5053568  -2.1173592  -4.2590604  -2.5307138 ]\n",
      " [ 3.5013475  -2.1105323  -4.2613835  -2.5282187 ]\n",
      " [ 3.4786353  -2.064704   -4.1821737  -2.577097  ]\n",
      " [ 3.5025983  -2.051344   -4.441217   -2.534398  ]\n",
      " [ 3.3893085  -1.8480105  -5.097487   -2.331954  ]\n",
      " [ 3.4979486  -2.1377597  -4.128482   -2.5455143 ]\n",
      " [ 3.4635024  -2.1674652  -3.9097917  -2.527371  ]\n",
      " [ 3.4806457  -2.188782   -3.9807909  -2.5047753 ]\n",
      " [ 3.4684992  -2.2157393  -3.8210952  -2.5220482 ]\n",
      " [ 3.3085637  -2.0380757  -3.477565   -2.5490208 ]\n",
      " [ 3.3251402  -2.1780472  -3.2696333  -2.5232685 ]\n",
      " [ 3.3448029  -2.318225   -3.1513307  -2.462022  ]\n",
      " [-0.30966106  3.0959668  -1.5355648  -2.6633387 ]\n",
      " [-0.72920525  2.9009938  -0.7394014  -1.8862379 ]\n",
      " [-1.5390601   0.5075873   3.8862936   0.35707685]\n",
      " [-1.2291467   0.23889056  3.8675034   0.22925788]\n",
      " [-0.40150756 -1.5325377  -0.71396714  2.6570752 ]\n",
      " [-0.33554277  3.1289377  -1.4647367  -2.6717844 ]]\n",
      "Argmax of predicted node label: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3 1]\n",
      "Predicted node label:  1\n",
      "Explain module for 26 nodes.\n",
      "Explain module with model :\n",
      " GcnEncoderNode(\n",
      "  (conv_first): GraphConv()\n",
      "  (conv_block): ModuleList(\n",
      "    (0): GraphConv()\n",
      "  )\n",
      "  (conv_last): GraphConv()\n",
      "  (act): ReLU()\n",
      "  (pred_model): Linear(in_features=60, out_features=4, bias=True)\n",
      "  (celoss): CrossEntropyLoss()\n",
      ")\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0135, 0.9765, 0.0074, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0135, 0.9765, 0.0074, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  0 ; loss:  3.57283878326416 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0135, 0.9765, 0.0074, 0.0026], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0115, 0.9786, 0.0075, 0.0024], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0115, 0.9786, 0.0075, 0.0024], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  1 ; loss:  3.5706069469451904 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0115, 0.9786, 0.0075, 0.0024], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0103, 0.9804, 0.0070, 0.0022], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0103, 0.9804, 0.0070, 0.0022], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  2 ; loss:  3.5687625408172607 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0103, 0.9804, 0.0070, 0.0022], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0098, 0.9820, 0.0062, 0.0020], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0098, 0.9820, 0.0062, 0.0020], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  3 ; loss:  3.567168951034546 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0098, 0.9820, 0.0062, 0.0020], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0093, 0.9834, 0.0055, 0.0018], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0093, 0.9834, 0.0055, 0.0018], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  4 ; loss:  3.56577467918396 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0093, 0.9834, 0.0055, 0.0018], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0088, 0.9845, 0.0049, 0.0017], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0088, 0.9845, 0.0049, 0.0017], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  5 ; loss:  3.5645976066589355 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0088, 0.9845, 0.0049, 0.0017], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0083, 0.9854, 0.0046, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0083, 0.9854, 0.0046, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  6 ; loss:  3.563693046569824 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0083, 0.9854, 0.0046, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0077, 0.9862, 0.0045, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0077, 0.9862, 0.0045, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  7 ; loss:  3.5629196166992188 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0077, 0.9862, 0.0045, 0.0016], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0072, 0.9868, 0.0045, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0072, 0.9868, 0.0045, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  8 ; loss:  3.56233549118042 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0072, 0.9868, 0.0045, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0067, 0.9873, 0.0046, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0067, 0.9873, 0.0046, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  9 ; loss:  3.5618345737457275 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0067, 0.9873, 0.0046, 0.0015], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0063, 0.9877, 0.0046, 0.0014], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0063, 0.9877, 0.0046, 0.0014], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  10 ; loss:  3.5614423751831055 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0063, 0.9877, 0.0046, 0.0014], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0060, 0.9880, 0.0047, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0060, 0.9880, 0.0047, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  11 ; loss:  3.5610616207122803 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0060, 0.9880, 0.0047, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0057, 0.9884, 0.0046, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0057, 0.9884, 0.0046, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  12 ; loss:  3.5606892108917236 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0057, 0.9884, 0.0046, 0.0013], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0056, 0.9887, 0.0045, 0.0012], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0056, 0.9887, 0.0045, 0.0012], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  13 ; loss:  3.560370683670044 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0056, 0.9887, 0.0045, 0.0012], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0055, 0.9890, 0.0044, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0055, 0.9890, 0.0044, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  14 ; loss:  3.560059070587158 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0055, 0.9890, 0.0044, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0054, 0.9893, 0.0042, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0054, 0.9893, 0.0042, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  15 ; loss:  3.559762954711914 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0054, 0.9893, 0.0042, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0054, 0.9896, 0.0040, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0054, 0.9896, 0.0040, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  16 ; loss:  3.559514045715332 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0054, 0.9896, 0.0040, 0.0011], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0053, 0.9898, 0.0038, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0053, 0.9898, 0.0038, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  17 ; loss:  3.5592494010925293 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0053, 0.9898, 0.0038, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0053, 0.9900, 0.0037, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0053, 0.9900, 0.0037, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  18 ; loss:  3.5590739250183105 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0053, 0.9900, 0.0037, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0053, 0.9902, 0.0036, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0053, 0.9902, 0.0036, 0.0010], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  19 ; loss:  3.5589141845703125 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0053, 0.9902, 0.0036, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0051, 0.9904, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0051, 0.9904, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  20 ; loss:  3.5587105751037598 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0051, 0.9904, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0049, 0.9906, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0049, 0.9906, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  21 ; loss:  3.5585010051727295 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0049, 0.9906, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0048, 0.9907, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0048, 0.9907, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  22 ; loss:  3.5583386421203613 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0048, 0.9907, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([0.0046, 0.9909, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([0.0046, 0.9909, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  23 ; loss:  3.5581836700439453 ; mask density:  0.7284994721412659 ; pred:  tensor([0.0046, 0.9909, 0.0035, 0.0010], grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.4917e-03, 9.9103e-01, 3.4903e-03, 9.8693e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.4917e-03, 9.9103e-01, 3.4903e-03, 9.8693e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  24 ; loss:  3.558026075363159 ; mask density:  0.7284994721412659 ; pred:  tensor([4.4917e-03, 9.9103e-01, 3.4903e-03, 9.8693e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.3972e-03, 9.9118e-01, 3.4465e-03, 9.7535e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.3972e-03, 9.9118e-01, 3.4465e-03, 9.7535e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  25 ; loss:  3.557874917984009 ; mask density:  0.7284994721412659 ; pred:  tensor([4.3972e-03, 9.9118e-01, 3.4465e-03, 9.7535e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.3207e-03, 9.9133e-01, 3.3873e-03, 9.6072e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.3207e-03, 9.9133e-01, 3.3873e-03, 9.6072e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  26 ; loss:  3.5577232837677 ; mask density:  0.7284994721412659 ; pred:  tensor([4.3207e-03, 9.9133e-01, 3.3873e-03, 9.6072e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.2592e-03, 9.9148e-01, 3.3163e-03, 9.4332e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.2592e-03, 9.9148e-01, 3.3163e-03, 9.4332e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  27 ; loss:  3.5575718879699707 ; mask density:  0.7284994721412659 ; pred:  tensor([4.2592e-03, 9.9148e-01, 3.3163e-03, 9.4332e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.2099e-03, 9.9163e-01, 3.2371e-03, 9.2475e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.2099e-03, 9.9163e-01, 3.2371e-03, 9.2475e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  28 ; loss:  3.5574235916137695 ; mask density:  0.7284994721412659 ; pred:  tensor([4.2099e-03, 9.9163e-01, 3.2371e-03, 9.2475e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.1745e-03, 9.9177e-01, 3.1491e-03, 9.0585e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.1745e-03, 9.9177e-01, 3.1491e-03, 9.0585e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  29 ; loss:  3.5572800636291504 ; mask density:  0.7284994721412659 ; pred:  tensor([4.1745e-03, 9.9177e-01, 3.1491e-03, 9.0585e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.1429e-03, 9.9191e-01, 3.0642e-03, 8.8649e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.1429e-03, 9.9191e-01, 3.0642e-03, 8.8649e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  30 ; loss:  3.557143211364746 ; mask density:  0.7284994721412659 ; pred:  tensor([4.1429e-03, 9.9191e-01, 3.0642e-03, 8.8649e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.1083e-03, 9.9203e-01, 2.9899e-03, 8.6725e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.1083e-03, 9.9203e-01, 2.9899e-03, 8.6725e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  31 ; loss:  3.557013988494873 ; mask density:  0.7284994721412659 ; pred:  tensor([4.1083e-03, 9.9203e-01, 2.9899e-03, 8.6725e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.0536e-03, 9.9216e-01, 2.9403e-03, 8.4795e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.0536e-03, 9.9216e-01, 2.9403e-03, 8.4795e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  32 ; loss:  3.556889533996582 ; mask density:  0.7284994721412659 ; pred:  tensor([4.0536e-03, 9.9216e-01, 2.9403e-03, 8.4795e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([4.0010e-03, 9.9227e-01, 2.9009e-03, 8.2950e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([4.0010e-03, 9.9227e-01, 2.9009e-03, 8.2950e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  33 ; loss:  3.5567781925201416 ; mask density:  0.7284994721412659 ; pred:  tensor([4.0010e-03, 9.9227e-01, 2.9009e-03, 8.2950e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.9328e-03, 9.9238e-01, 2.8791e-03, 8.1226e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.9328e-03, 9.9238e-01, 2.8791e-03, 8.1226e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  34 ; loss:  3.5566699504852295 ; mask density:  0.7284994721412659 ; pred:  tensor([3.9328e-03, 9.9238e-01, 2.8791e-03, 8.1226e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.8551e-03, 9.9248e-01, 2.8724e-03, 7.9654e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.8551e-03, 9.9248e-01, 2.8724e-03, 7.9654e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  35 ; loss:  3.5565693378448486 ; mask density:  0.7284994721412659 ; pred:  tensor([3.8551e-03, 9.9248e-01, 2.8724e-03, 7.9654e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.7470e-03, 9.9259e-01, 2.8792e-03, 7.8418e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.7470e-03, 9.9259e-01, 2.8792e-03, 7.8418e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  36 ; loss:  3.556454658508301 ; mask density:  0.7284994721412659 ; pred:  tensor([3.7470e-03, 9.9259e-01, 2.8792e-03, 7.8418e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.6905e-03, 9.9267e-01, 2.8666e-03, 7.7221e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.6905e-03, 9.9267e-01, 2.8666e-03, 7.7221e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  37 ; loss:  3.556373119354248 ; mask density:  0.7284994721412659 ; pred:  tensor([3.6905e-03, 9.9267e-01, 2.8666e-03, 7.7221e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.6208e-03, 9.9276e-01, 2.8560e-03, 7.6093e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.6208e-03, 9.9276e-01, 2.8560e-03, 7.6093e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  38 ; loss:  3.5562808513641357 ; mask density:  0.7284994721412659 ; pred:  tensor([3.6208e-03, 9.9276e-01, 2.8560e-03, 7.6093e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.5461e-03, 9.9286e-01, 2.8435e-03, 7.5027e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.5461e-03, 9.9286e-01, 2.8435e-03, 7.5027e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  39 ; loss:  3.5561821460723877 ; mask density:  0.7284994721412659 ; pred:  tensor([3.5461e-03, 9.9286e-01, 2.8435e-03, 7.5027e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4982e-03, 9.9295e-01, 2.8158e-03, 7.3991e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4982e-03, 9.9295e-01, 2.8158e-03, 7.3991e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  40 ; loss:  3.556095600128174 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4982e-03, 9.9295e-01, 2.8158e-03, 7.3991e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4648e-03, 9.9303e-01, 2.7730e-03, 7.2966e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4648e-03, 9.9303e-01, 2.7730e-03, 7.2966e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  41 ; loss:  3.5560085773468018 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4648e-03, 9.9303e-01, 2.7730e-03, 7.2966e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4433e-03, 9.9312e-01, 2.7183e-03, 7.1957e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4433e-03, 9.9312e-01, 2.7183e-03, 7.1957e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  42 ; loss:  3.5559215545654297 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4433e-03, 9.9312e-01, 2.7183e-03, 7.1957e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4299e-03, 9.9320e-01, 2.6565e-03, 7.0976e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4299e-03, 9.9320e-01, 2.6565e-03, 7.0976e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  43 ; loss:  3.5558362007141113 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4299e-03, 9.9320e-01, 2.6565e-03, 7.0976e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4250e-03, 9.9328e-01, 2.5920e-03, 7.0018e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4250e-03, 9.9328e-01, 2.5920e-03, 7.0018e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  44 ; loss:  3.5557565689086914 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4250e-03, 9.9328e-01, 2.5920e-03, 7.0018e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.4064e-03, 9.9337e-01, 2.5370e-03, 6.9154e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.4064e-03, 9.9337e-01, 2.5370e-03, 6.9154e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  45 ; loss:  3.555673837661743 ; mask density:  0.7284994721412659 ; pred:  tensor([3.4064e-03, 9.9337e-01, 2.5370e-03, 6.9154e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.3811e-03, 9.9344e-01, 2.4919e-03, 6.8341e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.3811e-03, 9.9344e-01, 2.4919e-03, 6.8341e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  46 ; loss:  3.5555949211120605 ; mask density:  0.7284994721412659 ; pred:  tensor([3.3811e-03, 9.9344e-01, 2.4919e-03, 6.8341e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.3531e-03, 9.9352e-01, 2.4533e-03, 6.7631e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.3531e-03, 9.9352e-01, 2.4533e-03, 6.7631e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  47 ; loss:  3.555520534515381 ; mask density:  0.7284994721412659 ; pred:  tensor([3.3531e-03, 9.9352e-01, 2.4533e-03, 6.7631e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.3260e-03, 9.9359e-01, 2.4181e-03, 6.6983e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.3260e-03, 9.9359e-01, 2.4181e-03, 6.6983e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  48 ; loss:  3.5554513931274414 ; mask density:  0.7284994721412659 ; pred:  tensor([3.3260e-03, 9.9359e-01, 2.4181e-03, 6.6983e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.2739e-03, 9.9367e-01, 2.3952e-03, 6.6393e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.2739e-03, 9.9367e-01, 2.3952e-03, 6.6393e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  49 ; loss:  3.5553698539733887 ; mask density:  0.7284994721412659 ; pred:  tensor([3.2739e-03, 9.9367e-01, 2.3952e-03, 6.6393e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.2228e-03, 9.9374e-01, 2.3794e-03, 6.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.2228e-03, 9.9374e-01, 2.3794e-03, 6.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  50 ; loss:  3.5552971363067627 ; mask density:  0.7284994721412659 ; pred:  tensor([3.2228e-03, 9.9374e-01, 2.3794e-03, 6.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.1691e-03, 9.9381e-01, 2.3677e-03, 6.5273e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.1691e-03, 9.9381e-01, 2.3677e-03, 6.5273e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  51 ; loss:  3.5552256107330322 ; mask density:  0.7284994721412659 ; pred:  tensor([3.1691e-03, 9.9381e-01, 2.3677e-03, 6.5273e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.1154e-03, 9.9388e-01, 2.3578e-03, 6.4692e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.1154e-03, 9.9388e-01, 2.3578e-03, 6.4692e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  52 ; loss:  3.5551555156707764 ; mask density:  0.7284994721412659 ; pred:  tensor([3.1154e-03, 9.9388e-01, 2.3578e-03, 6.4692e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.0640e-03, 9.9395e-01, 2.3474e-03, 6.4083e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.0640e-03, 9.9395e-01, 2.3474e-03, 6.4083e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  53 ; loss:  3.5550873279571533 ; mask density:  0.7284994721412659 ; pred:  tensor([3.0640e-03, 9.9395e-01, 2.3474e-03, 6.4083e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([3.0169e-03, 9.9401e-01, 2.3347e-03, 6.3441e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([3.0169e-03, 9.9401e-01, 2.3347e-03, 6.3441e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  54 ; loss:  3.555020809173584 ; mask density:  0.7284994721412659 ; pred:  tensor([3.0169e-03, 9.9401e-01, 2.3347e-03, 6.3441e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.9724e-03, 9.9408e-01, 2.3207e-03, 6.2779e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.9724e-03, 9.9408e-01, 2.3207e-03, 6.2779e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  55 ; loss:  3.554955244064331 ; mask density:  0.7284994721412659 ; pred:  tensor([2.9724e-03, 9.9408e-01, 2.3207e-03, 6.2779e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.9372e-03, 9.9414e-01, 2.3029e-03, 6.2088e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.9372e-03, 9.9414e-01, 2.3029e-03, 6.2088e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  56 ; loss:  3.5548949241638184 ; mask density:  0.7284994721412659 ; pred:  tensor([2.9372e-03, 9.9414e-01, 2.3029e-03, 6.2088e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.9007e-03, 9.9421e-01, 2.2794e-03, 6.1402e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.9007e-03, 9.9421e-01, 2.2794e-03, 6.1402e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  57 ; loss:  3.5548274517059326 ; mask density:  0.7284994721412659 ; pred:  tensor([2.9007e-03, 9.9421e-01, 2.2794e-03, 6.1402e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.8724e-03, 9.9427e-01, 2.2530e-03, 6.0698e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.8724e-03, 9.9427e-01, 2.2530e-03, 6.0698e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  58 ; loss:  3.5547657012939453 ; mask density:  0.7284994721412659 ; pred:  tensor([2.8724e-03, 9.9427e-01, 2.2530e-03, 6.0698e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.8410e-03, 9.9433e-01, 2.2315e-03, 5.9937e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.8410e-03, 9.9433e-01, 2.2315e-03, 5.9937e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  59 ; loss:  3.5547046661376953 ; mask density:  0.7284994721412659 ; pred:  tensor([2.8410e-03, 9.9433e-01, 2.2315e-03, 5.9937e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.8146e-03, 9.9439e-01, 2.2073e-03, 5.9176e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.8146e-03, 9.9439e-01, 2.2073e-03, 5.9176e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  60 ; loss:  3.5546460151672363 ; mask density:  0.7284994721412659 ; pred:  tensor([2.8146e-03, 9.9439e-01, 2.2073e-03, 5.9176e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.7854e-03, 9.9444e-01, 2.1851e-03, 5.8462e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.7854e-03, 9.9444e-01, 2.1851e-03, 5.8462e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  61 ; loss:  3.5545873641967773 ; mask density:  0.7284994721412659 ; pred:  tensor([2.7854e-03, 9.9444e-01, 2.1851e-03, 5.8462e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.7617e-03, 9.9450e-01, 2.1582e-03, 5.7789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.7617e-03, 9.9450e-01, 2.1582e-03, 5.7789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  62 ; loss:  3.5545296669006348 ; mask density:  0.7284994721412659 ; pred:  tensor([2.7617e-03, 9.9450e-01, 2.1582e-03, 5.7789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.7396e-03, 9.9456e-01, 2.1307e-03, 5.7138e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.7396e-03, 9.9456e-01, 2.1307e-03, 5.7138e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  63 ; loss:  3.5544731616973877 ; mask density:  0.7284994721412659 ; pred:  tensor([2.7396e-03, 9.9456e-01, 2.1307e-03, 5.7138e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.7176e-03, 9.9461e-01, 2.1037e-03, 5.6514e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.7176e-03, 9.9461e-01, 2.1037e-03, 5.6514e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  64 ; loss:  3.554417610168457 ; mask density:  0.7284994721412659 ; pred:  tensor([2.7176e-03, 9.9461e-01, 2.1037e-03, 5.6514e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.6957e-03, 9.9467e-01, 2.0780e-03, 5.5922e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.6957e-03, 9.9467e-01, 2.0780e-03, 5.5922e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  65 ; loss:  3.55436372756958 ; mask density:  0.7284994721412659 ; pred:  tensor([2.6957e-03, 9.9467e-01, 2.0780e-03, 5.5922e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.6711e-03, 9.9472e-01, 2.0550e-03, 5.5275e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.6711e-03, 9.9472e-01, 2.0550e-03, 5.5275e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  66 ; loss:  3.554309606552124 ; mask density:  0.7284994721412659 ; pred:  tensor([2.6711e-03, 9.9472e-01, 2.0550e-03, 5.5275e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.6454e-03, 9.9477e-01, 2.0343e-03, 5.4671e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.6454e-03, 9.9477e-01, 2.0343e-03, 5.4671e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  67 ; loss:  3.5542569160461426 ; mask density:  0.7284994721412659 ; pred:  tensor([2.6454e-03, 9.9477e-01, 2.0343e-03, 5.4671e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.6179e-03, 9.9483e-01, 2.0157e-03, 5.4107e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.6179e-03, 9.9483e-01, 2.0157e-03, 5.4107e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  68 ; loss:  3.5542047023773193 ; mask density:  0.7284994721412659 ; pred:  tensor([2.6179e-03, 9.9483e-01, 2.0157e-03, 5.4107e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.5887e-03, 9.9488e-01, 1.9989e-03, 5.3578e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.5887e-03, 9.9488e-01, 1.9989e-03, 5.3578e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  69 ; loss:  3.5541534423828125 ; mask density:  0.7284994721412659 ; pred:  tensor([2.5887e-03, 9.9488e-01, 1.9989e-03, 5.3578e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.5565e-03, 9.9493e-01, 1.9859e-03, 5.3038e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.5565e-03, 9.9493e-01, 1.9859e-03, 5.3038e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  70 ; loss:  3.5541024208068848 ; mask density:  0.7284994721412659 ; pred:  tensor([2.5565e-03, 9.9493e-01, 1.9859e-03, 5.3038e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.5260e-03, 9.9498e-01, 1.9734e-03, 5.2528e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.5260e-03, 9.9498e-01, 1.9734e-03, 5.2528e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  71 ; loss:  3.554054021835327 ; mask density:  0.7284994721412659 ; pred:  tensor([2.5260e-03, 9.9498e-01, 1.9734e-03, 5.2528e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.4886e-03, 9.9503e-01, 1.9642e-03, 5.2155e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.4886e-03, 9.9503e-01, 1.9642e-03, 5.2155e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  72 ; loss:  3.5540034770965576 ; mask density:  0.7284994721412659 ; pred:  tensor([2.4886e-03, 9.9503e-01, 1.9642e-03, 5.2155e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.4557e-03, 9.9507e-01, 1.9530e-03, 5.1781e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.4557e-03, 9.9507e-01, 1.9530e-03, 5.1781e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  73 ; loss:  3.553955316543579 ; mask density:  0.7284994721412659 ; pred:  tensor([2.4557e-03, 9.9507e-01, 1.9530e-03, 5.1781e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.4264e-03, 9.9512e-01, 1.9388e-03, 5.1394e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.4264e-03, 9.9512e-01, 1.9388e-03, 5.1394e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  74 ; loss:  3.553907871246338 ; mask density:  0.7284994721412659 ; pred:  tensor([2.4264e-03, 9.9512e-01, 1.9388e-03, 5.1394e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.4009e-03, 9.9517e-01, 1.9214e-03, 5.0985e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.4009e-03, 9.9517e-01, 1.9214e-03, 5.0985e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  75 ; loss:  3.553860664367676 ; mask density:  0.7284994721412659 ; pred:  tensor([2.4009e-03, 9.9517e-01, 1.9214e-03, 5.0985e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.3797e-03, 9.9521e-01, 1.9011e-03, 5.0548e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.3797e-03, 9.9521e-01, 1.9011e-03, 5.0548e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  76 ; loss:  3.553814649581909 ; mask density:  0.7284994721412659 ; pred:  tensor([2.3797e-03, 9.9521e-01, 1.9011e-03, 5.0548e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.3565e-03, 9.9526e-01, 1.8822e-03, 5.0080e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.3565e-03, 9.9526e-01, 1.8822e-03, 5.0080e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  77 ; loss:  3.553767442703247 ; mask density:  0.7284994721412659 ; pred:  tensor([2.3565e-03, 9.9526e-01, 1.8822e-03, 5.0080e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.3379e-03, 9.9531e-01, 1.8601e-03, 4.9603e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.3379e-03, 9.9531e-01, 1.8601e-03, 4.9603e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  78 ; loss:  3.5537219047546387 ; mask density:  0.7284994721412659 ; pred:  tensor([2.3379e-03, 9.9531e-01, 1.8601e-03, 4.9603e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.3213e-03, 9.9535e-01, 1.8366e-03, 4.9110e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.3213e-03, 9.9535e-01, 1.8366e-03, 4.9110e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  79 ; loss:  3.5536766052246094 ; mask density:  0.7284994721412659 ; pred:  tensor([2.3213e-03, 9.9535e-01, 1.8366e-03, 4.9110e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.3082e-03, 9.9539e-01, 1.8122e-03, 4.8623e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.3082e-03, 9.9539e-01, 1.8122e-03, 4.8623e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  80 ; loss:  3.55363392829895 ; mask density:  0.7284994721412659 ; pred:  tensor([2.3082e-03, 9.9539e-01, 1.8122e-03, 4.8623e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.2941e-03, 9.9544e-01, 1.7882e-03, 4.8134e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.2941e-03, 9.9544e-01, 1.7882e-03, 4.8134e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  81 ; loss:  3.553590774536133 ; mask density:  0.7284994721412659 ; pred:  tensor([2.2941e-03, 9.9544e-01, 1.7882e-03, 4.8134e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.2739e-03, 9.9548e-01, 1.7698e-03, 4.7667e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.2739e-03, 9.9548e-01, 1.7698e-03, 4.7667e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  82 ; loss:  3.5535473823547363 ; mask density:  0.7284994721412659 ; pred:  tensor([2.2739e-03, 9.9548e-01, 1.7698e-03, 4.7667e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.2563e-03, 9.9552e-01, 1.7493e-03, 4.7205e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.2563e-03, 9.9552e-01, 1.7493e-03, 4.7205e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  83 ; loss:  3.553504467010498 ; mask density:  0.7284994721412659 ; pred:  tensor([2.2563e-03, 9.9552e-01, 1.7493e-03, 4.7205e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.2388e-03, 9.9556e-01, 1.7308e-03, 4.6742e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.2388e-03, 9.9556e-01, 1.7308e-03, 4.6742e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  84 ; loss:  3.5534634590148926 ; mask density:  0.7284994721412659 ; pred:  tensor([2.2388e-03, 9.9556e-01, 1.7308e-03, 4.6742e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.2191e-03, 9.9560e-01, 1.7148e-03, 4.6286e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.2191e-03, 9.9560e-01, 1.7148e-03, 4.6286e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  85 ; loss:  3.5534231662750244 ; mask density:  0.7284994721412659 ; pred:  tensor([2.2191e-03, 9.9560e-01, 1.7148e-03, 4.6286e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.1972e-03, 9.9564e-01, 1.7010e-03, 4.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.1972e-03, 9.9564e-01, 1.7010e-03, 4.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  86 ; loss:  3.553382635116577 ; mask density:  0.7284994721412659 ; pred:  tensor([2.1972e-03, 9.9564e-01, 1.7010e-03, 4.5836e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.1740e-03, 9.9568e-01, 1.6887e-03, 4.5388e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.1740e-03, 9.9568e-01, 1.6887e-03, 4.5388e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  87 ; loss:  3.553342580795288 ; mask density:  0.7284994721412659 ; pred:  tensor([2.1740e-03, 9.9568e-01, 1.6887e-03, 4.5388e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.1511e-03, 9.9572e-01, 1.6777e-03, 4.4947e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.1511e-03, 9.9572e-01, 1.6777e-03, 4.4947e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  88 ; loss:  3.5533041954040527 ; mask density:  0.7284994721412659 ; pred:  tensor([2.1511e-03, 9.9572e-01, 1.6777e-03, 4.4947e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.1206e-03, 9.9576e-01, 1.6724e-03, 4.4486e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.1206e-03, 9.9576e-01, 1.6724e-03, 4.4486e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  89 ; loss:  3.5532636642456055 ; mask density:  0.7284994721412659 ; pred:  tensor([2.1206e-03, 9.9576e-01, 1.6724e-03, 4.4486e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.0931e-03, 9.9580e-01, 1.6659e-03, 4.4039e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.0931e-03, 9.9580e-01, 1.6659e-03, 4.4039e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  90 ; loss:  3.553225040435791 ; mask density:  0.7284994721412659 ; pred:  tensor([2.0931e-03, 9.9580e-01, 1.6659e-03, 4.4039e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.0685e-03, 9.9584e-01, 1.6571e-03, 4.3604e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.0685e-03, 9.9584e-01, 1.6571e-03, 4.3604e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  91 ; loss:  3.5531868934631348 ; mask density:  0.7284994721412659 ; pred:  tensor([2.0685e-03, 9.9584e-01, 1.6571e-03, 4.3604e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.0477e-03, 9.9588e-01, 1.6441e-03, 4.3207e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.0477e-03, 9.9588e-01, 1.6441e-03, 4.3207e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  92 ; loss:  3.5531489849090576 ; mask density:  0.7284994721412659 ; pred:  tensor([2.0477e-03, 9.9588e-01, 1.6441e-03, 4.3207e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.0307e-03, 9.9591e-01, 1.6271e-03, 4.2843e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.0307e-03, 9.9591e-01, 1.6271e-03, 4.2843e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  93 ; loss:  3.5531113147735596 ; mask density:  0.7284994721412659 ; pred:  tensor([2.0307e-03, 9.9591e-01, 1.6271e-03, 4.2843e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([2.0163e-03, 9.9595e-01, 1.6084e-03, 4.2485e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([2.0163e-03, 9.9595e-01, 1.6084e-03, 4.2485e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  94 ; loss:  3.553074359893799 ; mask density:  0.7284994721412659 ; pred:  tensor([2.0163e-03, 9.9595e-01, 1.6084e-03, 4.2485e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([1.9995e-03, 9.9599e-01, 1.5922e-03, 4.2136e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([1.9995e-03, 9.9599e-01, 1.5922e-03, 4.2136e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  95 ; loss:  3.5530378818511963 ; mask density:  0.7284994721412659 ; pred:  tensor([1.9995e-03, 9.9599e-01, 1.5922e-03, 4.2136e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([1.9850e-03, 9.9602e-01, 1.5752e-03, 4.1789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([1.9850e-03, 9.9602e-01, 1.5752e-03, 4.1789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  96 ; loss:  3.5530025959014893 ; mask density:  0.7284994721412659 ; pred:  tensor([1.9850e-03, 9.9602e-01, 1.5752e-03, 4.1789e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([1.9681e-03, 9.9606e-01, 1.5595e-03, 4.1465e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([1.9681e-03, 9.9606e-01, 1.5595e-03, 4.1465e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  97 ; loss:  3.552966833114624 ; mask density:  0.7284994721412659 ; pred:  tensor([1.9681e-03, 9.9606e-01, 1.5595e-03, 4.1465e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([1.9514e-03, 9.9609e-01, 1.5456e-03, 4.1112e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([1.9514e-03, 9.9609e-01, 1.5456e-03, 4.1112e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  98 ; loss:  3.5529325008392334 ; mask density:  0.7284994721412659 ; pred:  tensor([1.9514e-03, 9.9609e-01, 1.5456e-03, 4.1112e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "Started Training ... for : 20\n",
      "Node prediction with softmax after one epoch : tensor([1.9355e-03, 9.9613e-01, 1.5314e-03, 4.0762e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "ypred for one epoch : tensor([1.9355e-03, 9.9613e-01, 1.5314e-03, 4.0762e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "epoch:  99 ; loss:  3.552898645401001 ; mask density:  0.7284994721412659 ; pred:  tensor([1.9355e-03, 9.9613e-01, 1.5314e-03, 4.0762e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------------------------\n",
      "\n",
      "----------------------\n",
      "ypred :  tensor([1.9355e-03, 9.9613e-01, 1.5314e-03, 4.0762e-04],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "pred_label :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3 1]\n",
      "node_idx_new :  20\n",
      "Completed training in  0.7264435291290283\n",
      "GNN Explainer prefix :\"BAGraph_base_hdim20_odim20_explain\".\n",
      "Saved adjacency matrix to \"masked_adj_BAGraph_base_hdim20_odim20_explain_node_idx_20_graph_idx_-1.npy\".\n"
     ]
    }
   ],
   "source": [
    "ret_masked_adj = explainer.explain(prog_args.explain_node, unconstrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_masked_adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin of Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "coeffs = {\n",
    "    \"size\": 0.005,\n",
    "    \"feat_size\": 1.0,\n",
    "    \"ent\": 1.0,\n",
    "    \"feat_ent\": 0.1,\n",
    "    \"grad\": 0,\n",
    "    \"lap\": 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "class Explainer:\n",
    "    def __init__(self, model, node_features, node_labels, edge_index=None, adj_matrix=None, edge_weight=None, adj_mode=True):\n",
    "        self.adj_mode = adj_mode\n",
    "        self.GNN_model = model\n",
    "        # self.data = data\n",
    "        self.node_features = np.asarray(node_features).squeeze()\n",
    "        self.labels = np.asarray(node_labels).squeeze()\n",
    "        self.num_nodes = np.size(self.node_features, 0)\n",
    "        self.edge_index = np.asarray(edge_index) if edge_index is not None else None\n",
    "        self.adj_matrix = np.asarray(adj_matrix).squeeze() if adj_matrix is not None else self.GenerateAdjacentMatrix()\n",
    "        self.neighbours = self.Generate_k_hop_Neighbours()\n",
    "\n",
    "    def GenerateAdjacentMatrix(self):\n",
    "        edge_index = self.edge_index\n",
    "        zeros = np.zeros((self.num_nodes, self.num_nodes))\n",
    "        for i in range(np.size(edge_index, 1)):\n",
    "            u = edge_index[0][i]\n",
    "            v = edge_index[1][i]\n",
    "            zeros[u][v] = 1\n",
    "        adj_matrix = zeros.copy()\n",
    "        return adj_matrix\n",
    "\n",
    "    def Generate_k_hop_Neighbours(self):\n",
    "        # walking from node to other nodes, maximum distance == n_hops (number of layers in GNN)\n",
    "        num_modules = 0\n",
    "        for _, _ in enumerate(self.GNN_model.modules()):\n",
    "            num_modules += 1\n",
    "        n_hops = 3\n",
    "        print('n_hops:', n_hops)\n",
    "        adj = torch.tensor(self.adj_matrix, dtype=float)\n",
    "        diag_mask = torch.ones(adj.shape[0], adj.shape[0]) - torch.eye(adj.shape[0])\n",
    "        adj = adj * diag_mask\n",
    "        hop_adj = pow_adj = adj\n",
    "        # find all the nodes that can be reached within n_hops fromm node_idx\n",
    "        print('adj.shape:',adj.shape)\n",
    "        print('adj:\\n',adj)\n",
    "        for i in range(n_hops-1):\n",
    "            pow_adj = pow_adj @ adj\n",
    "            hop_adj = hop_adj + pow_adj\n",
    "            # reset all values to one\n",
    "            hop_adj = (hop_adj > 0).float()\n",
    "\n",
    "        return hop_adj.cpu().numpy().astype(int)\n",
    "\n",
    "    def ExtractNeighbours(self, node_idx):\n",
    "        # neighbourhoods consisting of nbhs that can be reached within k_hops\n",
    "        print('self.neighbours.shape:',self.neighbours.shape)\n",
    "        neighbours = self.neighbours[node_idx]\n",
    "        num_of_nbhs = sum(neighbours)\n",
    "\n",
    "        new_node_idx_sub_adj = sum(neighbours[:node_idx])\n",
    "\n",
    "        # np.nonzero(nbh_indices)[1] returns the value type\n",
    "        nbh_indices = np.nonzero(neighbours)[0]\n",
    "        print('nbh_indices:', nbh_indices)\n",
    "\n",
    "        sub_node_feat = self.node_features[nbh_indices]\n",
    "\n",
    "        sub_labels = self.labels[nbh_indices]\n",
    "\n",
    "\n",
    "        sub_edge_index = []\n",
    "        if self.edge_index is not None:\n",
    "            u,v = self.edge_index\n",
    "            sub_edge_index = []\n",
    "            for i in len(u):\n",
    "                if u[i] in nbh_indices and v[i] in nbh_indices:\n",
    "                    sub_edge_index.append((u[i],v[i]))\n",
    "\n",
    "        # return ONLY matrix of nbhs of node_idx and their nbhs that are the nbhs of the node_idx\n",
    "        # computation matrix for GNNExplainer\n",
    "        sub_adj_matrix = self.adj_matrix[nbh_indices][:, nbh_indices]\n",
    "\n",
    "        return new_node_idx_sub_adj, sub_adj_matrix, sub_node_feat, \\\n",
    "               sub_labels, sub_edge_index, nbh_indices\n",
    "\n",
    "    def ExplainNode(self, node_idx, epochs=None):\n",
    "        print()\n",
    "        if epochs is None:\n",
    "            epochs = 200\n",
    "        # Find nbhs of node_idx\n",
    "        new_node_idx_sub_adj, sub_adj_matrix, sub_node_features, \\\n",
    "        sub_labels, sub_edge_index, nbh_indices = self.ExtractNeighbours(node_idx)\n",
    "\n",
    "        sub_node_features = np.expand_dims(sub_node_features, axis=0)\n",
    "        sub_adj_matrix =  np.expand_dims(sub_adj_matrix, axis=0)\n",
    "\n",
    "        x_features = torch.tensor(sub_node_features, dtype=torch.float)\n",
    "        computation_graph = torch.tensor(sub_adj_matrix, requires_grad=True, dtype=torch.float)\n",
    "        sub_labels = torch.tensor(sub_labels, dtype=torch.long)\n",
    "\n",
    "        # Create GNNExplainer Model\n",
    "        gnn_explainer = ExplainModule(computation_graph, x_features,\n",
    "                                      sub_labels, sub_edge_index,\n",
    "                                      new_node_idx_sub_adj, self.GNN_model,\n",
    "                                      adj_mode=self.adj_mode)\n",
    "\n",
    "        # evaluate the GNN_model\n",
    "        self.GNN_model.eval()\n",
    "        gnn_explainer.train()\n",
    "\n",
    "        # Get the predicted label from the GNN Model\n",
    "        # based on the computation graph and sub features selected\n",
    "        print('computation_graph.shape:',computation_graph.shape)\n",
    "        print('x_features.shape:',x_features.shape)\n",
    "        model_ypred, _ = self.GNN_model(x_features, computation_graph)\n",
    "        model_ypred = model_ypred.cpu().detach().numpy()\n",
    "        model_ypred = np.squeeze(model_ypred)\n",
    "        model_ypred_label = np.argmax(model_ypred, axis=1)[new_node_idx_sub_adj]\n",
    "        print('GNN_Model labels node {} as {}\\n'\n",
    "              'Now GNN_Model predicts the node {} as {} based on the computation graph and the node features'\n",
    "              .format(node_idx, self.labels[node_idx], node_idx, model_ypred_label))\n",
    "        compare = [self.labels[node_idx], model_ypred_label]\n",
    "\n",
    "        prev_ypred = None\n",
    "        for epoch in range(epochs):\n",
    "            gnn_explainer.zero_grad()\n",
    "            gnn_explainer.optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # use GNN_model predict the label after masking\n",
    "            # ypred is a 1D tensor of the pred for each label for node_idx\n",
    "            # node_feat is a 1D tensor of the node_idx features\n",
    "            ypred, node_feat = gnn_explainer(new_node_idx_sub_adj)\n",
    "\n",
    "            loss = gnn_explainer.loss(ypred, self.labels[node_idx], new_node_idx_sub_adj)\n",
    "            loss.backward()\n",
    "            gnn_explainer.optimizer.step()\n",
    "\n",
    "            # print('Epoch: {}, loss: {}, ypred: {}'.format(epoch, loss, ypred))\n",
    "\n",
    "            # if prev_ypred is not None:\n",
    "            #     if ypred[model_ypred_label] > 0.98 \\\n",
    "            #             and ypred[model_ypred_label] - prev_ypred[model_ypred_label] <= 0.0001:\n",
    "            #         print('difference: ',ypred[model_ypred_label] - prev_ypred[model_ypred_label])\n",
    "            #         break\n",
    "            prev_ypred = ypred\n",
    "\n",
    "        masked_edges = gnn_explainer.edge_mask.cpu().detach().numpy()\n",
    "        masked_feat = gnn_explainer.node_feat_mask.cpu().detach().numpy()\n",
    "        ypred = ypred.cpu().detach().numpy()\n",
    "        labels = np.argmax(ypred, axis=0)\n",
    "\n",
    "        masked_adj = gnn_explainer.masked_adj.cpu().detach().numpy().squeeze() \\\n",
    "                         * sub_adj_matrix.squeeze()\n",
    "\n",
    "        print('sub_adj_matrix: {}, masked_edges: {}, sub_node_features: {}'\n",
    "              .format(sub_adj_matrix.shape, masked_edges.shape, sub_node_features.squeeze().shape))\n",
    "\n",
    "        self.PlotSubGraph(masked_adj, masked_edges,\n",
    "                          sub_edge_index, new_node_idx_sub_adj, node_idx,\n",
    "                          feats=sub_node_features.squeeze(), labels=sub_labels.cpu().detach().numpy().squeeze(),\n",
    "                          threshold_num=12, adj_mode=self.adj_mode)\n",
    "        return masked_edges, masked_feat, compare\n",
    "\n",
    "    def PlotSubGraph(self, adj, masked_edges, edge_index, new_node_idx, node_idx, feats=None, labels=None, threshold_num=None, adj_mode=True):\n",
    "        G = nx.Graph()\n",
    "        G1 = nx.Graph()\n",
    "\n",
    "        weighted_edge_list = []\n",
    "        weighted_edge_list1 = []\n",
    "        if threshold_num is not None:\n",
    "            if adj_mode:\n",
    "                # this is for symmetric graphs: edges are repeated twice in adj\n",
    "                adj_threshold_num = threshold_num * 2\n",
    "                neigh_size = len(adj[adj > 0])\n",
    "                threshold_num = min(neigh_size, adj_threshold_num)\n",
    "                # sort in ascending and retrieve last threshold_num from the back\n",
    "                threshold = np.sort(adj[adj > 0])[-threshold_num]\n",
    "\n",
    "                num_nodes = len(adj[0])\n",
    "                weighted_edge_list = [\n",
    "                    (i, j, adj[i, j])\n",
    "                    for i in range(num_nodes)\n",
    "                    for j in range(num_nodes)\n",
    "                    # if adj[i, j] >= threshold\n",
    "                    if adj[i,j] > 0\n",
    "                ]\n",
    "\n",
    "                weighted_edge_list1 = [\n",
    "                    (i, j, adj[i, j])\n",
    "                    for i in range(num_nodes)\n",
    "                    for j in range(num_nodes)\n",
    "                    if adj[i, j] >= threshold\n",
    "                ]\n",
    "            else:\n",
    "                threshold = np.sort(masked_edges)[-threshold_num]\n",
    "                row, col = edge_index\n",
    "\n",
    "                for i in range(len(row)):\n",
    "                    if masked_edges[i] >= threshold:\n",
    "                        weighted_edge_list.append((row[i], col[i], masked_edges[i]))\n",
    "\n",
    "\n",
    "        G.add_weighted_edges_from(weighted_edge_list)\n",
    "\n",
    "        G1.add_weighted_edges_from(weighted_edge_list1)\n",
    "\n",
    "        if feats is not None:\n",
    "            for node in G.nodes():\n",
    "                G.nodes[node][\"feat\"] = feats[node]\n",
    "            for node in G1.nodes():\n",
    "                G1.nodes[node][\"feat\"] = feats[node]\n",
    "\n",
    "        node_labels = {}\n",
    "        node_labels1 = {}\n",
    "        if labels is not None:\n",
    "            for node in G.nodes():\n",
    "                G.nodes[node][\"label\"] = labels[node]\n",
    "                node_labels.update({node: labels[node]})\n",
    "            for node in G1.nodes():\n",
    "                G1.nodes[node][\"label\"] = labels[node]\n",
    "                node_labels1.update({node: labels[node]})\n",
    "\n",
    "        node_color = []\n",
    "        for node in G.nodes.data():\n",
    "            if node[0] == new_node_idx:\n",
    "                node_color.append('green')\n",
    "            else:\n",
    "                node_color.append('lightgrey')\n",
    "\n",
    "        node_color1 = []\n",
    "        for node in G1.nodes.data():\n",
    "            if node[0] == new_node_idx:\n",
    "                node_color1.append('orange')\n",
    "            else:\n",
    "                node_color1.append('grey')\n",
    "\n",
    "\n",
    "        edge_colors = [w for (u, v, w) in G.edges.data(\"weight\", default=1)]\n",
    "        edge_vmax = statistics.median_high(\n",
    "            [d for (u, v, d) in G.edges(data=\"weight\", default=1)]\n",
    "        )\n",
    "        min_color = min([d for (u, v, d) in G.edges(data=\"weight\", default=1)])\n",
    "        # color range: gray to black\n",
    "        edge_vmin = 2 * min_color - edge_vmax\n",
    "\n",
    "        edge_colors1 = [w for (u, v, w) in G1.edges.data(\"weight\", default=1)]\n",
    "        edge_vmax1 = statistics.median_high(\n",
    "            [d for (u, v, d) in G1.edges(data=\"weight\", default=1)]\n",
    "        )\n",
    "        min_color1 = min([d for (u, v, d) in G1.edges(data=\"weight\", default=1)])\n",
    "        # color range: gray to black\n",
    "        edge_vmin1 = 2 * min_color1 - edge_vmax1\n",
    "\n",
    "        #plt.close()\n",
    "        plt.figure()\n",
    "        pos = nx.spring_layout(G, k=0.5)\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_color)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=plt.get_cmap(\"Greys\"),\n",
    "                               edge_vmin=edge_vmin, edge_vmax=edge_vmax)\n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Create subdirectory if does not already exist\n",
    "        data_path = os.path.join(PROJECT_ROOT_DIR, SUBGRAPH_FOLDER)\n",
    "        if not os.path.isdir(data_path):\n",
    "            os.makedirs(data_path)\n",
    "        # plot G into plt and highlist node_idx in the explanation\n",
    "        plt.savefig('{}/subgraph_node_non_mask{}.png'.format(data_path, node_idx))\n",
    "\n",
    "        #plt.close()\n",
    "        plt.figure()\n",
    "        pos = nx.spring_layout(G1, k=0.5)\n",
    "        nx.draw_networkx_nodes(G1, pos, node_color=node_color1)\n",
    "        nx.draw_networkx_edges(G1, pos, edge_color=edge_colors1, edge_cmap=plt.get_cmap(\"Greys\"),\n",
    "                               edge_vmin=edge_vmin1, edge_vmax=edge_vmax1)\n",
    "        nx.draw_networkx_labels(G1, pos, labels=node_labels1)\n",
    "        plt.axis('off')\n",
    "        # plot G into plt and highlist node_idx to be explained as red\n",
    "        plt.savefig('{}/subgraph_node_masked_edges{}.png'.format(data_path, node_idx))\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "class ExplainModule(nn.Module):\n",
    "    def __init__(self, adj, node_features, labels, edge_index, node_idx, model, adj_mode=True):\n",
    "        super(ExplainModule, self).__init__()\n",
    "        self.adj_mode = adj_mode\n",
    "        # dim --> n*n, G_c\n",
    "        self.sub_adj = adj\n",
    "        # dim --> n*d, X_c\n",
    "        self.sub_node_feat = node_features\n",
    "        # dim --> n*1, Y_c\n",
    "        self.sub_labels = labels\n",
    "        # dim --> 2*E_s\n",
    "        self.sub_edge_index = edge_index\n",
    "        self.sub_node_idx = node_idx\n",
    "        self.GNN_model = model\n",
    "        self.lr = 0.1\n",
    "\n",
    "        # setup GNNExplainer\n",
    "        num_nodes = np.size(self.sub_adj, 1)\n",
    "        feat_dim = np.size(self.sub_node_feat, -1)\n",
    "        self.edge_mask = self.ConstructEdgeMask(num_nodes)\n",
    "        self.node_feat_mask = self.ConstructFeatureMask(feat_dim)\n",
    "        self.optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask], lr=self.lr)\n",
    "\n",
    "        # used to remove self loops\n",
    "        self.masked_adj = None\n",
    "        self.diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes)\n",
    "\n",
    "    def ConstructEdgeMask(self, num_nodes):\n",
    "        if self.adj_mode:\n",
    "            edge_mask = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
    "        else:\n",
    "            # uses edge_index\n",
    "            edge_mask = nn.Parameter(torch.FloatTensor(num_nodes))\n",
    "        std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "            2.0 / (num_nodes + num_nodes)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "                edge_mask.normal_(1.0, std)\n",
    "        return edge_mask\n",
    "\n",
    "    def ConstructFeatureMask(self, num_features):\n",
    "        if num_features < 1:\n",
    "            num_features = 1\n",
    "        feat_mask = nn.Parameter(torch.FloatTensor(num_features))\n",
    "        std = 0.1\n",
    "        with torch.no_grad():\n",
    "            feat_mask.normal_(1.0, std)\n",
    "        return feat_mask\n",
    "\n",
    "    def forward(self, node_idx):\n",
    "        edge_index = self.sub_edge_index\n",
    "        # mask features\n",
    "        masked_node_feat = self.MaskFeatures()\n",
    "\n",
    "        # get prediction based after masking using the GNN_Model\n",
    "        if self.adj_mode:\n",
    "            # print('hello:',self.sub_adjadj)\n",
    "            self.masked_adj = self.MaskEdges()\n",
    "            ypred, adj_att = self.GNN_model(masked_node_feat, self.masked_adj)\n",
    "        else:\n",
    "            edge_masks = self.MaskEdges()\n",
    "            # edge_index and edge_masks form the sub_masked_adj_matrix\n",
    "            ypred, adj_att = self.GNN_model(masked_node_feat, edge_index)\n",
    "\n",
    "        # node_idx_label_pred = ypred[node_idx,:]\n",
    "        # node_feat = feat[node_idx,:]\n",
    "        # # softmax rescale Tensor elements to range[0,1] that sums to 1\n",
    "        # node_idx_label_pred = nn.Softmax(dim=0)(node_idx_label_pred)\n",
    "\n",
    "        # softmax rescale Tensor elements to range[0,1] that sums to 1\n",
    "        model_ypred = ypred[-1][node_idx]\n",
    "        model_ypred = nn.Softmax(dim=0)(model_ypred)\n",
    "\n",
    "        return model_ypred, adj_att\n",
    "\n",
    "    def loss(self, ypred, model_pred_label, node_idx):\n",
    "        ypred = ypred.squeeze()\n",
    "        node_idx_pred = ypred\n",
    "\n",
    "        ypred_node_idx = node_idx_pred[model_pred_label]\n",
    "        # if predict well, value should be close to 1, otherwise 0\n",
    "        pred_loss = -torch.log(ypred_node_idx)\n",
    "\n",
    "        edge_mask = self.edge_mask.sigmoid()\n",
    "        edge_size_loss = coeffs[\"size\"] * torch.sum(edge_mask)\n",
    "        edge_mask_entropy = (-1) * edge_mask * torch.log(edge_mask) - (1-edge_mask) * torch.log(1-edge_mask)\n",
    "        edge_mask_entropy_loss = coeffs[\"ent\"] * torch.mean(edge_mask_entropy)\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.sigmoid()\n",
    "        node_feat_size_loss = coeffs[\"feat_size\"] * torch.mean(node_feat_mask)\n",
    "\n",
    "        # calculate Laplacian loss, do we actually need this?\n",
    "        # https://www.groundai.com/project/gnn-explainer-a-tool-for-post-hoc-explanation-of-graph-neural-networks/1#S3.E2\n",
    "        # Section 5.1 Laplacian regularization\n",
    "\n",
    "        loss = pred_loss + edge_size_loss + edge_mask_entropy_loss + node_feat_size_loss\n",
    "        return loss\n",
    "\n",
    "    def MaskEdges(self):\n",
    "        edge_mask = self.edge_mask\n",
    "        if not self.adj_mode:\n",
    "            edge_mask = torch.sigmoid(self.edge_mask)\n",
    "            return edge_mask\n",
    "        else:\n",
    "            # print('edge_mask:\\n', edge_mask)\n",
    "            edge_mask = torch.sigmoid(edge_mask)\n",
    "            # print('edge maskkk')\n",
    "            # print(edge_mask)\n",
    "            sym_edge_mask = (edge_mask + edge_mask.t()) / 2\n",
    "            # print('sym_edge_mask')\n",
    "            # print(sym_edge_mask)\n",
    "            # print(self.sub_adj * sym_edge_mask)\n",
    "            masked_adj = self.sub_adj * sym_edge_mask\n",
    "            return masked_adj * self.diag_mask\n",
    "\n",
    "    def MaskFeatures(self):\n",
    "        node_feat_mask = self.node_feat_mask\n",
    "        node_feat_mask = torch.sigmoid(node_feat_mask)\n",
    "        masked_node_feat = node_feat_mask * self.sub_node_feat\n",
    "        return masked_node_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "SUBGRAPH_FOLDER = \"explainSubgraphs\"\n",
    "\n",
    "node_features = data['feat']\n",
    "node_labels = data['label']\n",
    "adj_matrix = data['adj']\n",
    "\n",
    "explainer = Explainer(model, node_features, node_labels, adj_matrix=adj_matrix, adj_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, compare = explainer.ExplainNode(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = []\n",
    "house_tips = [24, 29, 34]\n",
    "house_BAconnectors = [20, 25, 30]\n",
    "sorted_union_node_idx = sorted(list(set(house_tips) | set(house_BAconnectors)))\n",
    "\n",
    "for i in sorted_union_node_idx: # range(20,25)\n",
    "    print(\"Explainer for node :\", i)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    _, _, compare = explainer.ExplainNode(i)\n",
    "    comparison.append(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for i in range(len(comparison)):\n",
    "    print(comparison[i][0], comparison [i][1])\n",
    "    if comparison[i][0] != comparison [i][1]:\n",
    "        error += 1\n",
    "error_rate = error / len(comparison)\n",
    "print('Model evaluating computational graph error rate:', error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All respective files are in placed to run the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# This notebook works with Pytorch geometric GNNExplainer for explaining node predictions\n",
    "# of Cora dataset and/or BA Graph with house motifs\n",
    "# Created by : Au Jit Seah\n",
    "# File owners : Au Jit Seah\n",
    "##########################################################################################\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic graphs for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File : syntheticSim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BA-shape with house motifs and setting roles that will be used as labels\n",
    "\"\"\"\n",
    "Builds a BA preferential attachment graph, with \"node index\" starting from \"start\"\n",
    "parameter and \"role_ids\" from \"role_start\" parameter\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "start       :    starting index of the shape\n",
    "width       :    int size of the graph (no. of nodes)\n",
    "role_start  :    starting index for the roles\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "graph       :    a ba graph, with ids beginning from start\n",
    "roles       :    list of the roles of the nodes (indexed starting from\n",
    "                 role_start) that will be used as labels\n",
    "\"\"\"\n",
    "def ba(start, width, role_start=0, m=5):\n",
    "    graph = nx.barabasi_albert_graph(width, m)\n",
    "    graph.add_nodes_from(range(start, start + width))\n",
    "    nids = sorted(graph)\n",
    "    mapping = {nid: start + i for i, nid in enumerate(nids)}\n",
    "    graph = nx.relabel_nodes(graph, mapping)\n",
    "    roles = [role_start for i in range(width)]\n",
    "    return graph, roles\n",
    "\n",
    "\"\"\"\n",
    "Builds a house-like graph/motif, with \"node index\" starting from \"start\"\n",
    "parameter and \"role_ids\" from \"role_start\" parameter\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "start       :    starting index for the shape\n",
    "role_start  :    starting index for the roles\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "graph       :    a house-like graph/motif, with ids beginning from start\n",
    "roles       :    list of the roles of the nodes (indexed starting at\n",
    "                 role_start) that will be used as labels\n",
    "\"\"\"\n",
    "def house(start, role_start=0):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(start, start + 5))\n",
    "    graph.add_edges_from(\n",
    "        [\n",
    "            (start, start + 1),\n",
    "            (start + 1, start + 2),\n",
    "            (start + 2, start + 3),\n",
    "            (start + 3, start),\n",
    "        ]\n",
    "    )\n",
    "    # graph.add_edges_from([(start, start + 2), (start + 1, start + 3)])\n",
    "    graph.add_edges_from([(start + 4, start), (start + 4, start + 1)])\n",
    "    roles = [role_start, role_start, role_start + 1, role_start + 1, role_start + 2]\n",
    "    return graph, roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a basis graph and attaches elements of the type in the list randomly along the basis.\n",
    "Possibility to add random edges afterwards.\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "width_basis       :      width (in terms of number of nodes) of the basis\n",
    "basis_type        :      \"ba\"\n",
    "shapes            :      list of shape list\n",
    "                         (1st arg  : type of shape,\n",
    "                         next args : args for building the shape except for the start)\n",
    "start             :      initial node label for the first node\n",
    "rdm_basis_plugins :      Boolean\n",
    "                         For the shapes to be attached randomly (True) or\n",
    "                         regularly (False) to the basis graph\n",
    "add_random_edges  :      no. of edges to randomly add on the structure\n",
    "m                 :      no. of new edges to attach to existing node (for BA graph)\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "basis             :      a networkx graph with the particular shape used as the base\n",
    "role_id           :      label for each role (eg. representing basis or edges)\n",
    "plugins           :      node ids whereby the motif graph will be attached to the basis\n",
    "\"\"\"\n",
    "# Eg. build_graph(20, \"ba\", [[\"house\"]], start=0, m=5)\n",
    "def build_graph(width_basis, basis_type, list_shapes, start=0,\n",
    "                rdm_basis_plugins=False,add_random_edges=0, m=5):\n",
    "    print(\"------ Building the Synthetic BA graph with 'House' motifs ------\")\n",
    "    # Build the BA graph start with 0 and number of nodes (width basis)\n",
    "    if basis_type == \"ba\":\n",
    "        # Drawing of a house motif\n",
    "        basis, role_id = eval(basis_type)(start, width_basis, m=m)\n",
    "        print(\"Role Id of the BA graph :\\n\", role_id)\n",
    "#     else:\n",
    "#         # Drawing other type of motif\n",
    "#         basis, role_id = eval(basis_type)(start, width_basis)\n",
    "\n",
    "    n_basis, n_shapes = nx.number_of_nodes(basis), len(list_shapes)\n",
    "    start += n_basis  # indicator of the id of the next node\n",
    "    print(\"Indicator of the id of the next node :\", start)\n",
    "    \n",
    "    # role_id are '0's for all the nodes of the basis, BA graph\n",
    "    print(\"Number of nodes in the BA graph : \", n_basis)\n",
    "    print(\"Number of motifs : \", n_shapes)\n",
    "\n",
    "    print(\"List of shapes :\", list_shapes)\n",
    "    print(\"No. of shapes :\", len(list_shapes))\n",
    "\n",
    "    # Sample (with replacement) where to attach the new motifs\n",
    "    if rdm_basis_plugins is True:\n",
    "        plugins = np.random.choice(n_basis, n_shapes, replace=False)\n",
    "    else:\n",
    "        spacing = math.floor(n_basis / n_shapes)\n",
    "        print(\"Spacing : \", spacing)\n",
    "        plugins = [int(k * spacing) for k in range(n_shapes)]\n",
    "        print(\"Plugins : \", plugins)\n",
    "    seen_shapes = {\"basis\": [0, n_basis]}\n",
    "    print(\"seen_shapes : \", seen_shapes)\n",
    "    \n",
    "    for shape_index, shape in enumerate(list_shapes):\n",
    "        shape_type = shape[0]\n",
    "        print(\"\\n-----------------------------------------\")\n",
    "        print(\"Shape_ID : \" + str(shape_index) + \" with shape type : \" + str(shape_type))\n",
    "        print(str(len(shape)) + \" shapes with list of Shape :\", shape)\n",
    "        print(\"The shape starts from index 1 : \", shape[1:])\n",
    "        \n",
    "        args = [start]\n",
    "        \n",
    "        # More than one shape\n",
    "        if len(shape) > 1:\n",
    "            args += shape[1:]\n",
    "        \n",
    "        # Append 0 for the \"role_start\" in \"house\" function\n",
    "        args += [0]\n",
    "        print(\"\\nThe list of arguments :\", args)\n",
    "        # *args parameter to send a non-keyworded variable-length argument list to function, 1-2 parameters in this case\n",
    "        print(\"The first item in list of arguments :\", args[0])\n",
    "        print(\"The second item in list of arguments :\", args[1])\n",
    "        \n",
    "        # Creating the \"house\" motif\n",
    "        graph_s, roles_graph_s = eval(shape_type)(*args)\n",
    "        n_s = nx.number_of_nodes(graph_s)\n",
    "        \n",
    "        try:\n",
    "             # Get the last seen label from first index\n",
    "            col_start = seen_shapes[shape_type][0]\n",
    "        except:\n",
    "            # Get the max label value 1\n",
    "            col_start = np.max(role_id) + 1\n",
    "            # Add the new shape_type to the seen_shapes dictionary\n",
    "            seen_shapes[shape_type] = [col_start, n_s]\n",
    "        print(\"Column start :\", col_start)\n",
    "        print(\"Observe seen_shapes : \", seen_shapes)\n",
    "        \n",
    "        \n",
    "        # Attach the shape to the basis, BA graph\n",
    "        basis.add_nodes_from(graph_s.nodes())\n",
    "        basis.add_edges_from(graph_s.edges())\n",
    "        # Connecting the motif to the BA graph from node 20 to 0, 25 to 6 and 30 to 12\n",
    "        basis.add_edges_from([(start, plugins[shape_index])])\n",
    "#         if shape_type == \"cycle\":\n",
    "#             if np.random.random() > 0.5:\n",
    "#                 a = np.random.randint(1, 4)\n",
    "#                 b = np.random.randint(1, 4)\n",
    "#                 basis.add_edges_from([(a + start, b + plugins[shape_id])])\n",
    "\n",
    "        # start = 0; col_start = 1; roles_graph_s = [0, 0, 1, 1, 2]\n",
    "        temp_labels = [r + col_start for r in roles_graph_s]\n",
    "        # temp_labels increment roles_graph_s by col_start\n",
    "\n",
    "        # temp_labels[0] += 100 * seen_shapes[shape_type][0]\n",
    "        \n",
    "        # role_id is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        # Append labels of motif to the labels of BA graph\n",
    "        role_id += temp_labels\n",
    "        print(\"Labels of BA graph with attached motifs :\\n\", role_id)\n",
    "        print(\"No. of nodes in attached graph : \", nx.number_of_nodes(basis))\n",
    "        start += n_s\n",
    "        print(\"With attached motif nodes, index starts from : \", start)\n",
    "\n",
    "#     if add_random_edges > 0:\n",
    "#         # add random edges between nodes:\n",
    "#         for p in range(add_random_edges):\n",
    "#             src, dest = np.random.choice(nx.number_of_nodes(basis), 2, replace=False)\n",
    "#             print(src, dest)\n",
    "#             basis.add_edges_from([(src, dest)])\n",
    "\n",
    "    # Plotting the basis \"BA\" graph\n",
    "    # plt.figure(figsize=(8, 6), dpi=300)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title('BA graph'.upper(), y=1.0, fontsize=14)\n",
    "    nx.draw(basis, with_labels=True, font_weight='bold')\n",
    "\n",
    "    # Plot the motif \"house\" graph\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.title('\"House\" motif', y=1.0, fontsize=12)\n",
    "    nx.draw(graph_s, with_labels=True, font_weight='bold')\n",
    "    print(\"\\nInformation of the motif graph :\\n\", nx.info(graph_s))\n",
    "    \n",
    "    plt.show()\n",
    "            \n",
    "    return basis, role_id, plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File : featureGen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating node features\n",
    "import abc\n",
    "\n",
    "class FeatureGen(metaclass=abc.ABCMeta):\n",
    "    # Feature Generator base class from Abstract Base Classes\n",
    "    @abc.abstractmethod\n",
    "    def gen_node_features(self, G):\n",
    "        pass\n",
    "\n",
    "class ConstFeatureGen(FeatureGen):\n",
    "    # Generate constant node features in class\n",
    "    def __init__(self, val):\n",
    "        print(\"Values in Constant Feature Generator : \", val)\n",
    "        self.val = val\n",
    "\n",
    "    def gen_node_features(self, G):\n",
    "        print(\"Generate node features for \" + str(len(G.nodes())) + \" nodes.\")\n",
    "        feat_dict = {i:{'feat': np.array(self.val, dtype=np.float32)} for i in G.nodes()}\n",
    "        print('Values of feat_dict[0][\"feat\"]:', feat_dict[0]['feat'])\n",
    "        \n",
    "        # Set node attributes with values in feature dictionary of values '1's\n",
    "        nx.set_node_attributes(G, feat_dict)\n",
    "        print('Node attributes of node \\'0\\', G.nodes[0][\"feat\"]:', G.nodes[0]['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic BA graph with \"house\" motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File : gengraph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up graph and create its adjacency matrix\n",
    "\"\"\"\n",
    "Perturb the list of (sparse) graphs by adding/removing edges.\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "graph_list           :      the list of graphs to be perturbed\n",
    "p                    :      proportion of added edges based on current number of edges.\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "perturbed_graph_list :      the list of graphs that are perturbed from the original graphs.\n",
    "\"\"\"\n",
    "def perturb(graph_list, p):\n",
    "    perturbed_graph_list = []\n",
    "    for G_original in graph_list:\n",
    "        G = G_original.copy()\n",
    "        edge_count = int(G.number_of_edges() * p)\n",
    "        # randomly add the edges between a pair of nodes without an edge.\n",
    "        for _ in range(edge_count):\n",
    "            while True:\n",
    "                u = np.random.randint(0, G.number_of_nodes())\n",
    "                v = np.random.randint(0, G.number_of_nodes())\n",
    "                if (not G.has_edge(u, v)) and (u != v):\n",
    "                    break\n",
    "            G.add_edge(u, v)\n",
    "        perturbed_graph_list.append(G)\n",
    "    return perturbed_graph_list\n",
    "\n",
    "\"\"\"\n",
    "Load an existing graph to be converted for the experiments.\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "G                        :      Networkx graph is the input for preprocessing\n",
    "labels                   :      corresponding node labels\n",
    "normalize_adj            :      Boolean\n",
    "                                returns a normalized adjacency matrix (True)\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "{\"adj\", \"feat\" \"labels\"} :  dictionary containing adjacency, node features and labels\n",
    "\"\"\"\n",
    "def preprocess_input_graph(G, labels, normalize_adj=False):\n",
    "    # Create the adjacency matrix for graph\n",
    "    adj = np.array(nx.to_numpy_matrix(G))\n",
    "    print(\"------ Preprocess Input graph ------\")\n",
    "    print(\"The shape of the adjacency matrix ('dxd') of input graph :\", adj.shape)\n",
    "\n",
    "    # If normalization is required\n",
    "#     if normalize_adj:\n",
    "#         # Create a diagonal array\n",
    "#         sqrt_deg = np.diag(1.0 / np.sqrt(np.sum(adj, axis=0, dtype=float).squeeze()))\n",
    "#         adj = np.matmul(np.matmul(sqrt_deg, adj), sqrt_deg)\n",
    "\n",
    "    # last index from 0 - 34\n",
    "    existing_node = list(G.nodes)[-1]\n",
    "    # Dimension of features\n",
    "    feat_dim = G.nodes[existing_node][\"feat\"].shape[0]\n",
    "    print(\"Feature dimensions of the last node '\" + str(existing_node) + \"' : \" + str(feat_dim))\n",
    "\n",
    "    # Initialize feature ndarray (dimension of number_of_nodes x feat_dim) \n",
    "    features = np.zeros((G.number_of_nodes(), feat_dim), dtype=float)\n",
    "    for idx, node_id in enumerate(G.nodes()):\n",
    "        features[idx, :] = G.nodes[node_id][\"feat\"]\n",
    "\n",
    "    # add batch dim by expanding the shape horizontally\n",
    "    adj = np.expand_dims(adj, axis=0)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    labels = np.expand_dims(labels, axis=0)    \n",
    "    print(\"The shape of the adjacency matrix after expansion :\", adj.shape)\n",
    "    print(\"The shape of the features matrix after expansion :\", features.shape)\n",
    "    print(\"The shape of the labels matrix after expansion :\", labels.shape)\n",
    "    \n",
    "    return {\"adj\": adj, \"feat\": features, \"labels\": labels}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generating Synthetic Graph for experimentation :\n",
    "- Barabasi-Albert base graph and attach the no. of \"house\" motifs\n",
    "\n",
    "Input parameters :\n",
    "----------------------------------------------------------------------------------------\n",
    "nb_shapes         :  the no. of shapes ('house' motifs) that should be added to the\n",
    "                     base graph\n",
    "width_basis       :  the no. of nodes of the basis graph (ie. 'BA' graph)\n",
    "feature_generator :  a `Feature Generator` for node features\n",
    "                     addition of constant features to nodes ('None')\n",
    "m                 :  no. of edges to be attached to existing node (for 'BA' graph)\n",
    "\n",
    "Return values :\n",
    "----------------------------------------------------------------------------------------\n",
    "G                 :  a generated networkx \"ba\" graph with attached \"house\" motifs\n",
    "role_id           :  a list with total number of nodes in the entire graph (base graph\n",
    "                     and  motifs).  role_id[i] is the ID of the role of node i.\n",
    "                     It is also the label used for training and predictions\n",
    "name              :  a graph identifier\n",
    "\"\"\"\n",
    "def gen_syn1(nb_shapes=3, width_basis=20, feature_generator=None, m=5):\n",
    "    basis_type = \"ba\"\n",
    "    list_shapes = [[\"house\"]] * nb_shapes\n",
    "\n",
    "    # synthetic_structsim\n",
    "    G, role_id, _ = build_graph(width_basis, basis_type, list_shapes, start=0, m=5)\n",
    "    G = perturb([G], 0.01)[0]\n",
    "\n",
    "    if feature_generator is None:\n",
    "        # feature generator\n",
    "        feature_generator = ConstFeatureGen(1)\n",
    "    \n",
    "    # Generate node features\n",
    "    feature_generator.gen_node_features(G)\n",
    "\n",
    "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
    "    \n",
    "    print(\"------ Generated the Synthetic BA graph with 'House' motifs ------\")\n",
    "    print(\"Name of generated graph :\", name)\n",
    "    return G, role_id, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File : models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# This is the basic Graph Convolution Network class inherited from torch.nn module\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        add_self=False,\n",
    "        normalize_embedding=False,\n",
    "        dropout=0.0,\n",
    "        bias=True,\n",
    "        gpu=True,\n",
    "        att=False,\n",
    "    ):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.att = att\n",
    "        self.add_self = add_self\n",
    "        self.dropout = dropout\n",
    "        if dropout > 0.001:\n",
    "            self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.normalize_embedding = normalize_embedding\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if not gpu:\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "            if add_self:\n",
    "                self.self_weight = nn.Parameter(\n",
    "                    torch.FloatTensor(input_dim, output_dim)\n",
    "                )\n",
    "            if att:\n",
    "                self.att_weight = nn.Parameter(torch.FloatTensor(input_dim, input_dim))\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim).cuda())\n",
    "            if add_self:\n",
    "                self.self_weight = nn.Parameter(\n",
    "                    torch.FloatTensor(input_dim, output_dim).cuda()\n",
    "                )\n",
    "            if att:\n",
    "                self.att_weight = nn.Parameter(\n",
    "                    torch.FloatTensor(input_dim, input_dim).cuda()\n",
    "                )\n",
    "        if bias:\n",
    "            if not gpu:\n",
    "                self.bias = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "            else:\n",
    "                self.bias = nn.Parameter(torch.FloatTensor(output_dim).cuda())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # This 'forward' has to be implemented to avoid unimplemented error in Convolution layers\n",
    "    def forward(self, x, adj):\n",
    "        if self.dropout > 0.001:\n",
    "            x = self.dropout_layer(x)\n",
    "        # deg = torch.sum(adj, -1, keepdim=True)\n",
    "        if self.att:\n",
    "            x_att = torch.matmul(x, self.att_weight)\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "            att = x_att @ x_att.permute(0, 2, 1)\n",
    "            # att = self.softmax(att)\n",
    "            adj = adj * att\n",
    "\n",
    "        y = torch.matmul(adj, x)\n",
    "        y = torch.matmul(y, self.weight)\n",
    "        if self.add_self:\n",
    "            self_emb = torch.matmul(x, self.self_weight)\n",
    "            y += self_emb\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "        if self.normalize_embedding:\n",
    "            y = F.normalize(y, p=2, dim=2)\n",
    "            # print(y[0][0])\n",
    "        return y, adj\n",
    "\n",
    "    \n",
    "# Build the convolution layers for the GCN Graph Encoder\n",
    "# This is where the node masks and embeddings are created with predictions in forward pass\n",
    "class GcnEncoderGraph(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        embedding_dim,           # output_dim\n",
    "        label_dim,               # num_classes\n",
    "        num_layers,              # num_gc_layers\n",
    "        pred_hidden_dims=[],\n",
    "        concat=True,\n",
    "        bn=True,\n",
    "        dropout=0.0,\n",
    "        add_self=False,\n",
    "        args=None,\n",
    "    ):\n",
    "      \n",
    "        super(GcnEncoderGraph, self).__init__()\n",
    "        self.concat = concat\n",
    "\n",
    "        # add_self = add_self\n",
    "        self.add_self = add_self\n",
    "\n",
    "        # This value will change from 'True' if it is provided by the caller function\n",
    "        self.bn = bn\n",
    "        self.num_layers = num_layers\n",
    "        self.num_aggs = 1\n",
    "\n",
    "        self.bias = True        \n",
    "        self.gpu = args.gpu\n",
    "        print(\"*** Check received batch_size argument :\", args.batch_size)\n",
    "        print(\"*** Batch normalization from caller (default : False) :\", bn)\n",
    "\n",
    "        if args.method == \"att\":\n",
    "            self.att = True\n",
    "        else:\n",
    "            self.att = False\n",
    "        if args is not None:\n",
    "            self.bias = args.bias\n",
    "\n",
    "        self.conv_first, self.conv_block, self.conv_last = self.build_conv_layers(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            num_layers,\n",
    "            add_self,\n",
    "            normalize=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "        self.label_dim = label_dim\n",
    "\n",
    "        if concat:\n",
    "            self.pred_input_dim = hidden_dim * (num_layers - 1) + embedding_dim\n",
    "        else:\n",
    "            self.pred_input_dim = embedding_dim\n",
    "        self.pred_model = self.build_pred_layers(\n",
    "            self.pred_input_dim, pred_hidden_dims, label_dim, num_aggs=self.num_aggs\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, GraphConv):\n",
    "                init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain(\"relu\"))\n",
    "                if m.att:\n",
    "                    init.xavier_uniform_(\n",
    "                        m.att_weight.data, gain=nn.init.calculate_gain(\"relu\")\n",
    "                    )\n",
    "                if m.add_self:\n",
    "                    init.xavier_uniform_(\n",
    "                        m.self_weight.data, gain=nn.init.calculate_gain(\"relu\")\n",
    "                    )\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def build_conv_layers(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        num_layers,\n",
    "        add_self,\n",
    "        normalize=False,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        conv_first = GraphConv(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            add_self=add_self,\n",
    "            normalize_embedding=normalize,\n",
    "            bias=self.bias,\n",
    "            gpu=self.gpu,\n",
    "            att=self.att,\n",
    "        )\n",
    "        conv_block = nn.ModuleList(\n",
    "            [\n",
    "                GraphConv(\n",
    "                    input_dim=hidden_dim,\n",
    "                    output_dim=hidden_dim,\n",
    "                    add_self=add_self,\n",
    "                    normalize_embedding=normalize,\n",
    "                    dropout=dropout,\n",
    "                    bias=self.bias,\n",
    "                    gpu=self.gpu,\n",
    "                    att=self.att,\n",
    "                )\n",
    "                for i in range(num_layers - 2)\n",
    "            ]\n",
    "        )\n",
    "        conv_last = GraphConv(\n",
    "            input_dim=hidden_dim,\n",
    "            output_dim=embedding_dim,\n",
    "            add_self=add_self,\n",
    "            normalize_embedding=normalize,\n",
    "            bias=self.bias,\n",
    "            gpu=self.gpu,\n",
    "            att=self.att,\n",
    "        )\n",
    "        return conv_first, conv_block, conv_last\n",
    "\n",
    "    def build_pred_layers(\n",
    "        self, pred_input_dim, pred_hidden_dims, label_dim, num_aggs=1\n",
    "    ):\n",
    "        pred_input_dim = pred_input_dim * num_aggs\n",
    "        if len(pred_hidden_dims) == 0:\n",
    "            pred_model = nn.Linear(pred_input_dim, label_dim)\n",
    "        else:\n",
    "            pred_layers = []\n",
    "            for pred_dim in pred_hidden_dims:\n",
    "                pred_layers.append(nn.Linear(pred_input_dim, pred_dim))\n",
    "                pred_layers.append(self.act)\n",
    "                pred_input_dim = pred_dim\n",
    "            pred_layers.append(nn.Linear(pred_dim, label_dim))\n",
    "            pred_model = nn.Sequential(*pred_layers)\n",
    "        return pred_model\n",
    "\n",
    "    \"\"\"\n",
    "    For each num_nodes in batch_num_nodes, the first num_nodes entries of the \n",
    "    corresponding column are 1's, and the rest are 0's (to be masked out).\n",
    "    Dimension of mask: [batch_size x max_nodes x 1]\n",
    "    \"\"\"\n",
    "    def construct_mask(self, max_nodes, batch_num_nodes):\n",
    "        # masks\n",
    "        packed_masks = [torch.ones(int(num)) for num in batch_num_nodes]\n",
    "        batch_size = len(batch_num_nodes)\n",
    "        out_tensor = torch.zeros(batch_size, max_nodes)\n",
    "        for i, mask in enumerate(packed_masks):\n",
    "            out_tensor[i, : batch_num_nodes[i]] = mask\n",
    "        return out_tensor.unsqueeze(2).cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Batch normalization of 3D tensor x\n",
    "    \"\"\"\n",
    "    def apply_bn(self, x):\n",
    "        bn_module = nn.BatchNorm1d(x.size()[1])\n",
    "        if self.gpu:\n",
    "            bn_module = bn_module.cuda()\n",
    "        return bn_module(x)\n",
    "\n",
    "    \"\"\"\n",
    "    Perform forward prop with graph convolution.\n",
    "    Returns:\n",
    "        Embedding matrix with dimension [batch_size x num_nodes x embedding]\n",
    "        The embedding dim is self.pred_input_dim\n",
    "    \"\"\"\n",
    "    def gcn_forward(\n",
    "        self, x, adj, conv_first, conv_block, conv_last, embedding_mask=None\n",
    "    ):\n",
    "\n",
    "        x, adj_att = conv_first(x, adj)\n",
    "        x = self.act(x)\n",
    "        if self.bn:\n",
    "            x = self.apply_bn(x)\n",
    "        x_all = [x]\n",
    "        adj_att_all = [adj_att]\n",
    "        # out_all = []\n",
    "        # out, _ = torch.max(x, dim=1)\n",
    "        # out_all.append(out)\n",
    "        for i in range(len(conv_block)):\n",
    "            x, _ = conv_block[i](x, adj)\n",
    "            x = self.act(x)\n",
    "            if self.bn:\n",
    "                x = self.apply_bn(x)\n",
    "            x_all.append(x)\n",
    "            adj_att_all.append(adj_att)\n",
    "        x, adj_att = conv_last(x, adj)\n",
    "        x_all.append(x)\n",
    "        adj_att_all.append(adj_att)\n",
    "        # x_tensor: [batch_size x num_nodes x embedding]\n",
    "        x_tensor = torch.cat(x_all, dim=2)\n",
    "        if embedding_mask is not None:\n",
    "            x_tensor = x_tensor * embedding_mask\n",
    "        self.embedding_tensor = x_tensor\n",
    "\n",
    "        # adj_att_tensor: [batch_size x num_nodes x num_nodes x num_gc_layers]\n",
    "        adj_att_tensor = torch.stack(adj_att_all, dim=3)\n",
    "        return x_tensor, adj_att_tensor\n",
    "\n",
    "    # This 'forward' and 'loss' functions in 'GcnEncoderGraph' are not used\n",
    "#     def forward(self, x, adj, batch_num_nodes=None, **kwargs):\n",
    "#         # Embedding mask\n",
    "#         max_num_nodes = adj.size()[1]\n",
    "#         if batch_num_nodes is not None:\n",
    "#             self.embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
    "#         else:\n",
    "#             self.embedding_mask = None\n",
    "\n",
    "#         # Convolution\n",
    "#         x, adj_att = self.conv_first(x, adj)\n",
    "#         x = self.act(x)\n",
    "#         if self.bn:\n",
    "#             x = self.apply_bn(x)\n",
    "#         out_all = []\n",
    "#         out, _ = torch.max(x, dim=1)\n",
    "#         out_all.append(out)\n",
    "#         adj_att_all = [adj_att]\n",
    "#         for i in range(self.num_layers - 2):\n",
    "#             x, adj_att = self.conv_block[i](x, adj)\n",
    "#             x = self.act(x)\n",
    "#             if self.bn:\n",
    "#                 x = self.apply_bn(x)\n",
    "#             out, _ = torch.max(x, dim=1)\n",
    "#             out_all.append(out)\n",
    "#             if self.num_aggs == 2:\n",
    "#                 out = torch.sum(x, dim=1)\n",
    "#                 out_all.append(out)\n",
    "#             adj_att_all.append(adj_att)\n",
    "#         x, adj_att = self.conv_last(x, adj)\n",
    "#         adj_att_all.append(adj_att)\n",
    "#         # x = self.act(x)\n",
    "#         out, _ = torch.max(x, dim=1)\n",
    "#         out_all.append(out)\n",
    "#         if self.num_aggs == 2:\n",
    "#             out = torch.sum(x, dim=1)\n",
    "#             out_all.append(out)\n",
    "#         if self.concat:\n",
    "#             output = torch.cat(out_all, dim=1)\n",
    "#         else:\n",
    "#             output = out\n",
    "\n",
    "#         # adj_att_tensor: [batch_size x num_nodes x num_nodes x num_gc_layers]\n",
    "#         adj_att_tensor = torch.stack(adj_att_all, dim=3)\n",
    "\n",
    "#         self.embedding_tensor = output\n",
    "#         ypred = self.pred_model(output)\n",
    "#         # print(output.size())\n",
    "#         return ypred, adj_att_tensor\n",
    "\n",
    "#     def loss(self, pred, label, type=\"softmax\"):\n",
    "#         # softmax + CE\n",
    "#         if type == \"softmax\":\n",
    "#             return F.cross_entropy(pred, label, size_average=True)\n",
    "#         elif type == \"margin\":\n",
    "#             batch_size = pred.size()[0]\n",
    "#             label_onehot = torch.zeros(batch_size, self.label_dim).long().cuda()\n",
    "#             label_onehot.scatter_(1, label.view(-1, 1), 1)\n",
    "#             return torch.nn.MultiLabelMarginLoss()(pred, label_onehot)\n",
    "\n",
    "        # return F.binary_cross_entropy(F.sigmoid(pred[:,0]), label.float())\n",
    "\n",
    "# GCN Encoding of the nodes\n",
    "class GcnEncoderNode(GcnEncoderGraph):    \n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, label_dim, num_layers,\n",
    "                 pred_hidden_dims=[], concat=True,\n",
    "                 bn=True, dropout=0.0, args=None,):\n",
    "        super(GcnEncoderNode, self).__init__(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            label_dim,\n",
    "            num_layers,\n",
    "            pred_hidden_dims,\n",
    "            concat,\n",
    "            bn,\n",
    "            dropout,\n",
    "            args=args,\n",
    "        )\n",
    "        \n",
    "        if hasattr(args, \"loss_weight\"):\n",
    "            print(\"Loss weight: \", args.loss_weight)\n",
    "            self.celoss = nn.CrossEntropyLoss(weight=args.loss_weight)\n",
    "        else:\n",
    "            self.celoss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, adj, batch_num_nodes=None, **kwargs):\n",
    "        # Embedding masks\n",
    "        max_num_nodes = adj.size()[1]\n",
    "        if batch_num_nodes is not None:\n",
    "            embedding_mask = self.construct_mask(max_num_nodes, batch_num_nodes)\n",
    "        else:\n",
    "            embedding_mask = None\n",
    "\n",
    "        self.adj_atts = []\n",
    "        \n",
    "         # Using 'gcn_forward' rather than 'forward' in GcnEncoderGraph\n",
    "        self.embedding_tensor, adj_att = self.gcn_forward(\n",
    "            x, adj, self.conv_first, self.conv_block, self.conv_last, embedding_mask\n",
    "        )\n",
    "        pred = self.pred_model(self.embedding_tensor)\n",
    "        return pred, adj_att\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        pred = torch.transpose(pred, 1, 2)\n",
    "        return self.celoss(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File : train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate node classifications\n",
    "def evaluate_node(ypred, labels, train_idx, test_idx):\n",
    "    _, pred_labels = torch.max(ypred, 2)\n",
    "    pred_labels = pred_labels.numpy()\n",
    "\n",
    "    pred_train = np.ravel(pred_labels[:, train_idx])\n",
    "    pred_test = np.ravel(pred_labels[:, test_idx])\n",
    "    labels_train = np.ravel(labels[:, train_idx])\n",
    "    labels_test = np.ravel(labels[:, test_idx])\n",
    "\n",
    "    result_train = {\n",
    "        \"prec\": metrics.precision_score(labels_train, pred_train, average=\"macro\"),\n",
    "        \"recall\": metrics.recall_score(labels_train, pred_train, average=\"macro\"),\n",
    "        \"acc\": metrics.accuracy_score(labels_train, pred_train),\n",
    "        \"conf_mat\": metrics.confusion_matrix(labels_train, pred_train),\n",
    "    }\n",
    "    result_test = {\n",
    "        \"prec\": metrics.precision_score(labels_test, pred_test, average=\"macro\"),\n",
    "        \"recall\": metrics.recall_score(labels_test, pred_test, average=\"macro\"),\n",
    "        \"acc\": metrics.accuracy_score(labels_test, pred_test),\n",
    "        \"conf_mat\": metrics.confusion_matrix(labels_test, pred_test),\n",
    "    }\n",
    "    return result_train, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train node classifier and save the prediction results\n",
    "def train_node_classifier(G, labels, model, args, writer=None):\n",
    "    # train/test split only for nodes\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Training data with 80% ratio, labels_train.size()\n",
    "    num_train = int(num_nodes * args.train_ratio)\n",
    "    idx = [i for i in range(num_nodes)]\n",
    "\n",
    "    # Shuffle for training\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:num_train]\n",
    "    test_idx = idx[num_train:]\n",
    "\n",
    "    # data = gengraph.preprocess_input_graph(G, labels)\n",
    "    data = preprocess_input_graph(G, labels)\n",
    "    labels_train = torch.tensor(data[\"labels\"][:, train_idx], dtype=torch.long)\n",
    "    adj = torch.tensor(data[\"adj\"], dtype=torch.float)\n",
    "    x = torch.tensor(data[\"feat\"], requires_grad=True, dtype=torch.float)\n",
    "\n",
    "    \n",
    "#     scheduler, optimizer = train_utils.build_optimizer(\n",
    "#         args, model.parameters(), weight_decay=args.weight_decay\n",
    "#     )\n",
    "    # list(testModel.parameters()) and list(filter_fn) to show contents\n",
    "    # train_utils.build_optimizer \n",
    "    filter_fn = filter(lambda p : p.requires_grad, model.parameters())\n",
    "\n",
    "    # args.opt == 'adam':\n",
    "    optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=0.0)\n",
    "    scheduler = None\n",
    "\n",
    "    # Sets the module in training mode\n",
    "    model.train()\n",
    "    ypred = None\n",
    "    for epoch in range(args.num_epochs):\n",
    "        begin_time = time.time()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if args.gpu:\n",
    "            ypred, adj_att = model(x.cuda(), adj.cuda())\n",
    "        else:\n",
    "            ypred, adj_att = model(x, adj)\n",
    "        ypred_train = ypred[:, train_idx, :]\n",
    "        if args.gpu:\n",
    "            loss = model.loss(ypred_train, labels_train.cuda())\n",
    "        else:\n",
    "            loss = model.loss(ypred_train, labels_train)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        #for param_group in optimizer.param_groups:\n",
    "        #    print(param_group[\"lr\"])\n",
    "        elapsed = time.time() - begin_time\n",
    "\n",
    "        # Obtain with Confusion matrices for Train and Test results\n",
    "        result_train, result_test = evaluate_node(\n",
    "            ypred.cpu(), data[\"labels\"], train_idx, test_idx\n",
    "        )\n",
    "        \n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"loss/avg_loss\", loss, epoch)\n",
    "            writer.add_scalars(\n",
    "                \"prec\",\n",
    "                {\"train\": result_train[\"prec\"], \"test\": result_test[\"prec\"]},\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"recall\",\n",
    "                {\"train\": result_train[\"recall\"], \"test\": result_test[\"recall\"]},\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"acc\", {\"train\": result_train[\"acc\"], \"test\": result_test[\"acc\"]}, epoch\n",
    "            )\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                \"epoch: \",\n",
    "                epoch,\n",
    "                \"; loss: \",\n",
    "                loss.item(),\n",
    "                \"; train_acc: \",\n",
    "                result_train[\"acc\"],\n",
    "                \"; test_acc: \",\n",
    "                result_test[\"acc\"],\n",
    "                \"; train_prec: \",\n",
    "                result_train[\"prec\"],\n",
    "                \"; test_prec: \",\n",
    "                result_test[\"prec\"],\n",
    "                \"; epoch time: \",\n",
    "                \"{0:0.2f}\".format(elapsed),\n",
    "            )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(\"Confusion Matrix of train result :\\n\", result_train[\"conf_mat\"])\n",
    "    print(\"Confusion Matrix of test result :\\n\", result_test[\"conf_mat\"])\n",
    "\n",
    "    # Sets the module in evaluation mode for computational graph\n",
    "    model.eval()\n",
    "    if args.gpu:\n",
    "        ypred, _ = model(x.cuda(), adj.cuda())\n",
    "    else:\n",
    "        ypred, _ = model(x, adj)\n",
    "\n",
    "    cg_data = {\n",
    "        \"adj\": data[\"adj\"],\n",
    "        \"feat\": data[\"feat\"],\n",
    "        \"label\": data[\"labels\"],\n",
    "        \"pred\": ypred.cpu().detach().numpy(),\n",
    "        \"train_idx\": train_idx,\n",
    "    }\n",
    "    \n",
    "    print(\"Labels of the Computational graph :\\n\", cg_data['label'])\n",
    "    print(\"Prediction result of the Computational graph :\\n\", cg_data['pred'])\n",
    "    print(\"Train index of the Computational graph data :\\n\", cg_data['train_idx'])\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    \n",
    "    #io_utils.save_checkpoint\n",
    "    save_checkpoint(model, optimizer, args, num_epochs=-1, cg_dict=cg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# (1) GcnEncoderNode(GcnEncoderGraph) -> build_conv_layers -> GraphConv\n",
    "# build_conv_layers return conv_first, conv_block, conv_last\n",
    "# (2) GcnEncoderNode(GcnEncoderGraph) -> self.pred_model = self.build_pred_layers\n",
    "# build_pred_layers return pred_model\n",
    "# (3) syn_task1 -> train_node_classifier -> save_checkpoint\n",
    "#####################################################################################\n",
    "# Create the GCN model and encoding the nodes\n",
    "def syn_task1(args, writer=None):\n",
    "    # featgen.ConstFeatureGen\n",
    "    # np.ones(input_dim, dtype=float) = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
    "    constant_feature = ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
    "    print(\"Constant feature generator : \", constant_feature.val)\n",
    "    \n",
    "    #feat_dict = {i:{'feat': np.array(constant_feature.val, dtype=np.float32)} for i in G.nodes()}\n",
    "    #print ('Values of feat_dict[0][\"feat\"]:', feat_dict[0]['feat'])\n",
    "\n",
    "    #nx.set_node_attributes(G, feat_dict)\n",
    "    #print('Node attributes of node \\'0\\', G.nodes[0][\"feat\"]:', G.nodes[0]['feat'])\n",
    "\n",
    "    # gengraph.gen_syn1\n",
    "    # Create the BA graph with the \"house\" motifs\n",
    "    G, labels, name = gen_syn1(feature_generator=constant_feature)\n",
    "\n",
    "    # No .of classes from [0-3] for BA graph with house motifs\n",
    "    num_classes = max(labels) + 1\n",
    "    # Update number of classes in argument for training (Out of bounds error)\n",
    "    args.num_classes = num_classes\n",
    "    \n",
    "    # GcnEncoderNode model\n",
    "    print(\"------------ GCNEncoderNode Model ------------\")\n",
    "    print(\"Input dimensions :\", args.input_dim)\n",
    "    print(\"Hidden dimensions :\", args.hidden_dim)\n",
    "    print(\"Output dimensions :\", args.output_dim)\n",
    "    print(\"Number of classes in args :\", args.num_classes)\n",
    "    print(\"Number of GCN layers :\", args.num_gc_layers)\n",
    "    print(\"Method : \", args.method)\n",
    "\n",
    "    model = GcnEncoderNode(args.input_dim, args.hidden_dim, args.output_dim,\n",
    "                           args.num_classes, args.num_gc_layers, bn=args.bn, args=args)\n",
    "    \n",
    "    print(\"GcnEncoderNode model :\\n\", model)\n",
    "    \n",
    "\n",
    "#     if args.method == \"att\":\n",
    "#         print(\"Method: att\")\n",
    "#         model = models.GcnEncoderNode(\n",
    "#             args.input_dim,\n",
    "#             args.hidden_dim,\n",
    "#             args.output_dim,\n",
    "#             num_classes,\n",
    "#             args.num_gc_layers,\n",
    "#             bn=args.bn,\n",
    "#             args=args,\n",
    "#         )\n",
    "#     else:\n",
    "#         print(\"Method:\", args.method)\n",
    "#         model = models.GcnEncoderNode(\n",
    "#             args.input_dim,\n",
    "#             args.hidden_dim,\n",
    "#             args.output_dim,\n",
    "#             num_classes,\n",
    "#             args.num_gc_layers,\n",
    "#             bn=args.bn,\n",
    "#             args=args,\n",
    "#         )\n",
    "    if args.gpu:\n",
    "        model = model.cuda()\n",
    "\n",
    "    train_node_classifier(G, labels, model, args, writer=writer)\n",
    "    \n",
    "    # To be removed after testing\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "# Call flow 1 : syn_task1 -> gen_syn1 -> build_graph -> ba\n",
    "# Call flow 2 : syn_task1 -> GcnEncoderNode, train_node_classifier -> preprocess_input_graph\n",
    "model = syn_task1(prog_args, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for PyTorch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn-model-explainer/utils/io_utils.py\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda\n",
    "#prog_args.bn\n",
    "# Use `zero_division` parameter to control this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "## transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "## download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#print(len(trainset))\n",
    "#print(trainset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 28x28x1 => 26x26x32\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.d1 = nn.Linear(26 * 26 * 32, 128)\n",
    "        self.d2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 32x1x28x28 => 32x32x26x26\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # flatten => 32 x (32*26*26)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        #x = x.view(32, -1)\n",
    "\n",
    "        # 32 x (32*26*26) => 32x128\n",
    "        x = self.d1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # logits => 32x10\n",
    "        logits = self.d2(x)\n",
    "        out = F.softmax(logits, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.ones((2,2))\n",
    "\n",
    "ta = torch.tensor(a, dtype=float).to('cuda:0')\n",
    "tb = torch.ones(2,2, dtype=float).to('cuda:0')\n",
    "\n",
    "print(ta)\n",
    "print(tb)\n",
    "print(ta @ tb) # dot product; element-wise product\n",
    "\n",
    "# This takes awhile for CUDA configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device : \", device)\n",
    "\n",
    "model = MyModel()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    ## training step\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        ## forward + backprop + loss\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## update model params\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += (torch.argmax(logits, 1).flatten() == labels).type(torch.float).mean().item()\n",
    "    \n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
    "          %(epoch, train_running_loss / i, train_acc/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = 0.0\n",
    "for i, (images, labels) in enumerate(testloader, 0):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    test_acc += (torch.argmax(outputs, 1).flatten() == labels).type(torch.float).mean().item()\n",
    "    preds = torch.argmax(outputs, 1).flatten().cpu().numpy()\n",
    "        \n",
    "print('Test Accuracy: %.2f'%(test_acc/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.task = task\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        for l in range(2):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = 3\n",
    "\n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "        if self.task == 'node':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        else:\n",
    "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "          x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if not i == self.num_layers - 1:\n",
    "                x = self.lns[i](x)\n",
    "\n",
    "        if self.task == 'graph':\n",
    "            x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return emb, F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_self = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Removes every self-loop in the graph given by edge_index\n",
    "        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n",
    "\n",
    "        # Transform node feature matrix.\n",
    "        self_x = self.lin_self(x)\n",
    "        #x = self.lin(x)\n",
    "\n",
    "        return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index, size):\n",
    "        # Compute messages\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = pyg_utils.degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer):\n",
    "    if task == 'graph':\n",
    "        data_size = len(dataset)\n",
    "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        test_loader = loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(200):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            #print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = test(test_loader, model)\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "                epoch, total_loss, test_acc))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            emb, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        if model.task == 'node':\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate on nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "    \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset) \n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip3 install tensorboardX\n",
    "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "#!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ipython().system_raw(\n",
    "#    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "#    .format(\"./log\")\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ipython().system_raw('./ngrok http 6006 &')\n",
    "#!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "#    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph'\n",
    "\n",
    "print(\"Dataset :\", dataset)\n",
    "print(\"Length of dataset :\", len(dataset))\n",
    "print(\"Number of classes in dataset :\", dataset.num_classes)\n",
    "print(\"Number of node featues in dataset :\", dataset.num_node_features)\n",
    "\n",
    "# Have access to all 600 graphs in the datase\n",
    "data = dataset[0]\n",
    "print(\"\\nDataset :\", data)\n",
    "print(\"\\nIs the graph undirected :\", data.is_undirected())\n",
    "\n",
    "# Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
    "# Data(edge_index=[2, 44], x=[12, 3], y=[1])\n",
    "# The first graph in the dataset contains 12 nodes, each one having 3 features.\n",
    "# There are 44/2 = 22 undirected edges and the graph is assigned to exactly one class.\n",
    "# In addition, the data object is holding exactly one graph-level target.\n",
    "train_dataset = dataset[:540]\n",
    "test_dataset = dataset[540:]\n",
    "print(\"Accessing dataset [0-539] for training :\", train_dataset)\n",
    "print(\"Accessing dataset [540-599] for testing :\", test_dataset)\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "# Equivalent of the following\n",
    "# perm = torch.randperm(len(dataset))\n",
    "# dataset = dataset[perm]\n",
    "print(\"Shuffle dataset :\", dataset)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n----------- Model Training -----------\")\n",
    "model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "count = 1\n",
    "total_graphs = 0\n",
    "for data in loader:\n",
    "    # User batch to average node features in the node dimension for each graph individually\n",
    "    x = scatter_mean(data.x, data.batch, dim=0)\n",
    "    \n",
    "    if count == 1:\n",
    "        print(\"------------------------------ No. : \" + str(count) + \" ------------------------------\")\n",
    "        print(\"Batch :\\n\", data)\n",
    "        print(\"No. of batch graphs :\", data.num_graphs)\n",
    "        print(\"Size of mean x in batch :\\n\", x.size())  \n",
    "    count += 1\n",
    "    total_graphs += data.num_graphs\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Total no. of graphs in dataset :\", total_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch is a column vector which maps each node to its respective graph in the batch:\n",
    "data.to_data_list()[0] # ['edge_index'] gives the mapping of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = Planetoid(root='/tmp/cora', name='cora')\n",
    "task = 'node'\n",
    "\n",
    "print(\"Dataset :\\n\", dataset)\n",
    "print(\"Length of dataset :\", len(dataset))\n",
    "print(\"Number of classes in dataset :\", dataset.num_classes)\n",
    "print(\"Number of node featues in dataset :\", dataset.num_node_features)\n",
    "\n",
    "# Get a single, undirected citation graph from dataset\n",
    "data = dataset[0]\n",
    "print(\"\\nDataset :\\n\", data)\n",
    "print(\"Is single citation graph undirected :\", data.is_undirected())\n",
    "\n",
    "# data object holds a label for each node, and additional attributes :\n",
    "# train_mask denotes against which nodes to train (140 nodes)\n",
    "print(\"Number of items in train mask :\", data.train_mask.sum().item())\n",
    "# val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes)\n",
    "print(\"Number of items in val mask :\", data.val_mask.sum().item())\n",
    "# test_mask denotes against which nodes to test (1000 nodes)\n",
    "print(\"Number of items in test mask :\", data.test_mask.sum().item())\n",
    "\n",
    "# Train model\n",
    "print(\"\\n----------- Model Training -----------\")\n",
    "model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\"red\", \"orange\", \"green\", \"blue\", \"purple\", \"brown\",\n",
    "              \"black\", \"yellow\", \"grey\", \"cyan\", \"pink\", \"magenta\"]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "embs = []\n",
    "colors = []\n",
    "for batch in loader:\n",
    "    emb, pred = model(batch)\n",
    "    embs.append(emb)\n",
    "    colors += [color_list[y] for y in batch.y]\n",
    "embs = torch.cat(embs, dim=0)\n",
    "\n",
    "xs, ys = zip(*TSNE().fit_transform(embs.detach().numpy()))\n",
    "\n",
    "plt.figure(figsize=(24, 20))\n",
    "plt.scatter(xs, ys, color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph methods\n",
    "-  Graph Neural Network (GCN) <br>\n",
    "(http://tkipf.github.io/graph-convolutional-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.num_node_features, dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# A two-layer GCN\n",
    "# Non-linearity is not integrated in the conv calls and hence needs to be applied afterwards\n",
    "# Use ReLU as our intermediate non-linearity between and finally output a softmax distribution\n",
    "# over the number of classes\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net().to(device)\n",
    "print(\"Current device : \", device)\n",
    "\n",
    "# Get a single, undirected citation graph from dataset\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Explainer (pytorch_geometric/examples/gnn_explainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from pytorch_geometric/torch_geometric/nn/models/gnn_explainer.py\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class GNNExplainer(torch.nn.Module):\n",
    "    r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "    Explanations for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "    structures and small subsets node features that play a crucial role in a\n",
    "    GNN’s node-predictions.\n",
    "    .. note::\n",
    "        For an example of using GNN-Explainer, see `examples/gnn_explainer.py\n",
    "        <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "        gnn_explainer.py>`_.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "            progress. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'node_feat_size': 1.0,\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, epochs=100, lr=0.01, log=True):\n",
    "        super(GNNExplainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.log = log\n",
    "\n",
    "    def __set_masks__(self, x, edge_index, init=\"normal\"):\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        self.node_feat_mask = torch.nn.Parameter(torch.randn(F) * 0.1)\n",
    "\n",
    "        std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "        self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "\n",
    "    def __clear_masks__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.node_feat_masks = None\n",
    "        self.edge_mask = None\n",
    "\n",
    "    def __num_hops__(self):\n",
    "        num_hops = 0\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                num_hops += 1\n",
    "        return num_hops\n",
    "\n",
    "    def __flow__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def __subgraph__(self, node_idx, x, edge_index, **kwargs):\n",
    "        num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "\n",
    "        subset, edge_index, edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.__num_hops__(), edge_index, relabel_nodes=True,\n",
    "            num_nodes=num_nodes, flow=self.__flow__())\n",
    "\n",
    "        x = x[subset]\n",
    "        for key, item in kwargs:\n",
    "            if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                item = item[subset]\n",
    "            elif torch.is_tensor(item) and item.size(0) == num_edges:\n",
    "                item = item[edge_mask]\n",
    "            kwargs[key] = item\n",
    "\n",
    "        return x, edge_index, edge_mask, kwargs\n",
    "\n",
    "    def __loss__(self, node_idx, log_logits, pred_label):\n",
    "        loss = -log_logits[node_idx, pred_label[node_idx]]\n",
    "\n",
    "        m = self.edge_mask.sigmoid()\n",
    "        loss = loss + self.coeffs['edge_size'] * m.sum()\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        m = self.node_feat_mask.sigmoid()\n",
    "        loss = loss + self.coeffs['node_feat_size'] * m.sum()\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def explain_node(self, node_idx, x, edge_index, **kwargs):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for node\n",
    "        :attr:`node_idx`.\n",
    "        Args:\n",
    "            node_idx (int): The node to explain.\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        num_edges = edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        x, edge_index, hard_edge_mask, kwargs = self.__subgraph__(\n",
    "            node_idx, x, edge_index, **kwargs)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        with torch.no_grad():\n",
    "            log_logits = self.model(x=x, edge_index=edge_index, **kwargs)\n",
    "            pred_label = log_logits.argmax(dim=-1)\n",
    "\n",
    "        self.__set_masks__(x, edge_index)\n",
    "        self.to(x.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask],\n",
    "                                     lr=self.lr)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.epochs)\n",
    "            pbar.set_description(f'Explain node {node_idx}')\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.node_feat_mask.view(1, -1).sigmoid()\n",
    "            log_logits = self.model(x=h, edge_index=edge_index, **kwargs)\n",
    "            loss = self.__loss__(0, log_logits, pred_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.log:  # pragma: no cover\n",
    "                pbar.update(1)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.detach().sigmoid()\n",
    "        edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "        edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        return node_feat_mask, edge_mask\n",
    "\n",
    "    def visualize_subgraph(self, node_idx, edge_index, edge_mask, y=None,\n",
    "                           threshold=None, **kwargs):\n",
    "        r\"\"\"Visualizes the subgraph around :attr:`node_idx` given an edge mask\n",
    "        :attr:`edge_mask`.\n",
    "        Args:\n",
    "            node_idx (int): The node id to explain.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            edge_mask (Tensor): The edge mask.\n",
    "            y (Tensor, optional): The ground-truth node-prediction labels used\n",
    "                as node colorings. (default: :obj:`None`)\n",
    "            threshold (float, optional): Sets a threshold for visualizing\n",
    "                important edges. If set to :obj:`None`, will visualize all\n",
    "                edges with transparancy indicating the importance of edges.\n",
    "                (default: :obj:`None`)\n",
    "            **kwargs (optional): Additional arguments passed to\n",
    "                :func:`nx.draw`.\n",
    "        :rtype: :class:`matplotlib.pyplot`\n",
    "        \"\"\"\n",
    "\n",
    "        assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        subset, edge_index, hard_edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.__num_hops__(), edge_index, relabel_nodes=True,\n",
    "            num_nodes=None, flow=self.__flow__())\n",
    "\n",
    "        edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "        if threshold is not None:\n",
    "            edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "\n",
    "        if y is None:\n",
    "            y = torch.zeros(edge_index.max().item() + 1,\n",
    "                            device=edge_index.device)\n",
    "        else:\n",
    "            y = y[subset].to(torch.float) / y.max().item()\n",
    "\n",
    "        data = Data(edge_index=edge_index, att=edge_mask, y=y,\n",
    "                    num_nodes=y.size(0)).to('cpu')\n",
    "        G = to_networkx(data, node_attrs=['y'], edge_attrs=['att'])\n",
    "        mapping = {k: i for k, i in enumerate(subset.tolist())}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        kwargs['with_labels'] = kwargs.get('with_labels') or True\n",
    "        kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "        kwargs['node_size'] = kwargs.get('node_size') or 800\n",
    "        kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "        ax = plt.gca()\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            ax.annotate(\n",
    "                '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "                textcoords='data', arrowprops=dict(\n",
    "                    arrowstyle=\"->\",\n",
    "                    alpha=max(data['att'], 0.1),\n",
    "                    shrinkA=sqrt(kwargs['node_size']) / 2.0,\n",
    "                    shrinkB=sqrt(kwargs['node_size']) / 2.0,\n",
    "                    connectionstyle=\"arc3,rad=0.1\",\n",
    "                ))\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=y.tolist(), **kwargs)\n",
    "        nx.draw_networkx_labels(G, pos, **kwargs)\n",
    "        plt.axis('off')\n",
    "        return plt\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from pytorch_geometric/torch_geometric/utils/subgraph.py\n",
    "def maybe_num_nodes(index, num_nodes=None):\n",
    "    return index.max().item() + 1 if num_nodes is None else num_nodes\n",
    "\n",
    "def k_hop_subgraph(node_idx, num_hops, edge_index, relabel_nodes=False,\n",
    "                   num_nodes=None, flow='source_to_target'):\n",
    "    r\"\"\"Computes the :math:`k`-hop subgraph of :obj:`edge_index` around node\n",
    "    :attr:`node_idx`.\n",
    "    It returns (1) the nodes involved in the subgraph, (2) the filtered\n",
    "    :obj`edge_index` connectivity, and (3) the edge mask indicating which edges\n",
    "    were preserved.\n",
    "    Args:\n",
    "        node_idx (int): The central node.\n",
    "        num_hops: (int): The number of hops :math:`k`.\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        relabel_nodes (bool, optional): If set to :obj:`True`, the resulting\n",
    "            :obj:`edge_index` will be relabeled to hold consecutive indices\n",
    "            starting from zero. (default: :obj:`False`)\n",
    "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
    "        flow (string, optional): The flow direction of :math:`k`-hop\n",
    "            aggregation (:obj:`\"source_to_target\"` or\n",
    "            :obj:`\"target_to_source\"`). (default: :obj:`\"source_to_target\"`)\n",
    "    :rtype: (:class:`LongTensor`, :class:`LongTensor`, :class:`BoolTensor`)\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    if flow == 'target_to_source':\n",
    "        row, col = edge_index\n",
    "    else:\n",
    "        col, row = edge_index\n",
    "\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    subsets = [torch.tensor([node_idx], device=row.device).flatten()]\n",
    "    for _ in range(num_hops):\n",
    "        node_mask.fill_(False)\n",
    "        node_mask[subsets[-1]] = True\n",
    "        torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "        subsets.append(col[edge_mask])\n",
    "    subset = torch.cat(subsets).unique()\n",
    "    # Add `node_idx` to the beginning of `subset`.\n",
    "    subset = subset[subset != node_idx]\n",
    "    subset = torch.cat([torch.tensor([node_idx], device=row.device), subset])\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    if relabel_nodes:\n",
    "        node_idx = row.new_full((num_nodes, ), -1)\n",
    "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "        edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Sequential, Linear\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = '/tmp/cora'\n",
    "\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "\n",
    "# Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
    "data = dataset[0]\n",
    "print(\"Data before sent to device :\\n\", data)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin = Sequential(Linear(10, 10))\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device : \", device)\n",
    "\n",
    "model = Net().to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "print(\"Data to device :\\n\", data)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "x, edge_index = data.x, data.edge_index\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    log_logits = model(x, edge_index)\n",
    "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "explainer = GNNExplainer(model, epochs=200)\n",
    "node_idx = 10\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "plt = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder for unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "def unsupervised_train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    writer.add_scalar(\"loss\", loss.item(), epoch)\n",
    "\n",
    "def unsupervised_test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = Planetoid(\"/tmp/citeseer\", \"Citeseer\", T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "channels = 16\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA availability:', torch.cuda.is_available())\n",
    "\n",
    "# encoder: written by us; decoder: default (inner product)\n",
    "model = pyg_nn.GAE(Encoder(dataset.num_features, channels)).to(dev)\n",
    "labels = data.y\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "\n",
    "#data = model.split_edges(data)\n",
    "data = pyg_utils.train_test_split_edges(data)\n",
    "\n",
    "x, train_pos_edge_index = data.x.to(dev), data.train_pos_edge_index.to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    unsupervised_train(epoch)\n",
    "    auc, ap = unsupervised_test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    writer.add_scalar(\"AUC\", auc, epoch)\n",
    "    writer.add_scalar(\"AP\", ap, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = model.encode(x, train_pos_edge_index)\n",
    "colors = [color_list[y] for y in labels]\n",
    "\n",
    "xs, ys = zip(*TSNE().fit_transform(z.cpu().detach().numpy()))\n",
    "\n",
    "plt.figure(figsize=(24, 20))\n",
    "plt.scatter(xs, ys, color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
